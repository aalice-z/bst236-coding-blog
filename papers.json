{
  "last_updated": "2026-02-23T01:03:22.498031",
  "count": 20,
  "papers": [
    {
      "id": "2602.17642v1",
      "title": "A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning",
      "authors": [
        "Dhruv Talwar",
        "Harsh Desai",
        "Wendong Yin",
        "Goutam Mohanty",
        "Rafael Reveles"
      ],
      "abstract": "Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T18:54:06Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17642v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17642v1"
    },
    {
      "id": "2602.17321v1",
      "title": "The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound",
      "authors": [
        "Christoph Balada",
        "Aida Romano-Martinez",
        "Payal Varshney",
        "Vincent ten Cate",
        "Katharina Geschke",
        "Jonas Tesarz",
        "Paul Claßen",
        "Alexander K. Schuster",
        "Dativa Tibyampansha",
        "Karl-Patrik Kresoja",
        "Philipp S. Wild",
        "Sheraz Ahmed",
        "Andreas Dengel"
      ],
      "abstract": "Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T12:37:48Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17321v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17321v1"
    },
    {
      "id": "2602.17102v1",
      "title": "Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction",
      "authors": [
        "Sai Vineeth Kandappareddigari",
        "Santhoshkumar Jagadish",
        "Gauri Verma",
        "Ilhuicamina Contreras",
        "Christopher Dignam",
        "Anmol Srivastava",
        "Benjamin Demers"
      ],
      "abstract": "This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T05:59:55Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17102v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17102v1"
    },
    {
      "id": "2602.17085v1",
      "title": "ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions",
      "authors": [
        "Shogo Sato",
        "Kazuo Tanaka",
        "Shojun Ogasawara",
        "Kazuki Yamamoto",
        "Kazuhiko Murasaki",
        "Ryuichi Tanida",
        "Jun Kataoka"
      ],
      "abstract": "Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T05:11:41Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17085v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17085v1"
    },
    {
      "id": "2602.16908v1",
      "title": "Multi-objective optimization and quantum hybridization of equivariant deep learning interatomic potentials on organic and inorganic compounds",
      "authors": [
        "G. Laskaris",
        "D. Morozov",
        "D. Tarpanov",
        "A. Seth",
        "J. Procelewska",
        "G. Sai Gautam",
        "A. Sagingalieva",
        "R. Brasher",
        "A. Melnikov"
      ],
      "abstract": "Allegro is a machine learning interatomic potential (MLIP) model designed to predict atomic properties in molecules using E(3) equivariant neural networks. When training this model, there tends to be a trade-off between accuracy and inference time. For this reason we apply multi-objective hyperparameter optimization to the two objectives. Additionally, we experiment with modified architectures by making variants of Allegro some by adding strictly classical multi-layer perceptron (MLP) layers and some by adding quantum-classical hybrid layers. We compare the results from QM9, rMD17-aspirin, rMD17-benzene and our own proprietary dataset consisting of copper and lithium atoms. As results, we have a list of variants that surpass the Allegro in accuracy and also results which demonstrate the trade-off with inference times.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T21:48:35Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16908v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16908v1"
    },
    {
      "id": "2602.16696v1",
      "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks",
      "authors": [
        "Huan Souza",
        "Pankaj Mehta"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T18:42:29Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16696v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16696v1"
    },
    {
      "id": "2602.16548v1",
      "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
      "authors": [
        "Tianmeng Hu",
        "Yongzheng Cui",
        "Biao Luo",
        "Ke Li"
      ],
      "abstract": "The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T15:52:26Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16548v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16548v1"
    },
    {
      "id": "2602.16507v1",
      "title": "Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?",
      "authors": [
        "Gaetan De Waele",
        "Marek Wydmuch",
        "Krzysztof Dembczyński",
        "Wojciech Kotłowski",
        "Willem Waegeman"
      ],
      "abstract": "One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T14:48:08Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16507v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16507v1"
    },
    {
      "id": "2602.16468v2",
      "title": "HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting",
      "authors": [
        "Jung Min Choi",
        "Vijaya Krishna Yalavarthi",
        "Lars Schmidt-Thieme"
      ],
      "abstract": "In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T13:59:04Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16468v2.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16468v2"
    },
    {
      "id": "2602.16264v1",
      "title": "Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge",
      "authors": [
        "Zixian Wu",
        "Xuebao Li",
        "Yanfang Zheng",
        "Rui Wang",
        "Shunhuang Zhang",
        "Jinfang Wei",
        "Yongshang Lv",
        "Liang Dong",
        "Zamri Zainal Abidin",
        "Noraisyah Mohamed Shah",
        "Hongwei Ye",
        "Pengchao Yan",
        "Xuefeng Li",
        "Xiaojia Ji",
        "Xusheng Huang",
        "Xiaotian Wang",
        "Honglei Jin"
      ],
      "abstract": "In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T08:30:02Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16264v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16264v1"
    },
    {
      "id": "2602.16256v1",
      "title": "Color-based Emotion Representation for Speech Emotion Recognition",
      "authors": [
        "Ryotaro Nagase",
        "Ryoichi Takashima",
        "Yoichi Yamashita"
      ],
      "abstract": "Speech emotion recognition (SER) has traditionally relied on categorical or dimensional labels. However, this technique is limited in representing both the diversity and interpretability of emotions. To overcome this limitation, we focus on color attributes, such as hue, saturation, and value, to represent emotions as continuous and interpretable scores. We annotated an emotional speech corpus with color attributes via crowdsourcing and analyzed them. Moreover, we built regression models for color attributes in SER using machine learning and deep learning, and explored the multitask learning of color attribute regression and emotion classification. As a result, we demonstrated the relationship between color attributes and emotions in speech, and successfully developed color attribute regression models for SER. We also showed that multitask learning improved the performance of each task.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T08:11:49Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16256v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16256v1"
    },
    {
      "id": "2602.16224v1",
      "title": "Amortized Predictability-aware Training Framework for Time Series Forecasting and Classification",
      "authors": [
        "Xu Zhang",
        "Peng Wang",
        "Yichen Li",
        "Wei Wang"
      ],
      "abstract": "Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima. Therefore, mitigating the adverse effects of low-predictability samples is crucial for time series analysis tasks such as time series forecasting (TSF) and time series classification (TSC). While many deep learning models have achieved promising performance, few consider how to identify and penalize low-predictability samples to improve model performance from the training perspective. To fill this gap, we propose a general Amortized Predictability-aware Training Framework (APTF) for both TSF and TSC. APTF introduces two key designs that enable the model to focus on high-predictability samples while still learning appropriately from low-predictability ones: (i) a Hierarchical Predictability-aware Loss (HPL) that dynamically identifies low-predictability samples and progressively expands their loss penalty as training evolves, and (ii) an amortization model that mitigates predictability estimation errors caused by model bias, further enhancing HPL's effectiveness. The code is available at https://github.com/Meteor-Stars/APTF.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T06:59:05Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16224v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16224v1"
    },
    {
      "id": "2602.16216v1",
      "title": "UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection",
      "authors": [
        "Hamzeh Asgharnezhad",
        "Pegah Tabarisaadi",
        "Abbas Khosravi",
        "Roohallah Alizadehsani",
        "U. Rajendra Acharya"
      ],
      "abstract": "Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transformer encoders to process raw ECG signals and their spectrograms jointly. Evaluated on the MIT-BIH Arrhythmia and PTB Diagnostic datasets, UCTECG-Net outperforms LSTM, CNN1D, and Transformer baselines in terms of accuracy, precision, recall and F1 score, achieving up to 98.58% accuracy on MIT-BIH and 99.14% on PTB. To assess predictive reliability, we integrate three uncertainty quantification methods (Monte Carlo Dropout, Deep Ensembles, and Ensemble Monte Carlo Dropout) into all models and analyze their behavior using an uncertainty-aware confusion matrix and derived metrics. The results show that UCTECG-Net, particularly with Ensemble or EMCD, provides more reliable and better-aligned uncertainty estimates than competing architectures, offering a stronger basis for risk-aware ECG decision support.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T06:39:19Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16216v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16216v1"
    },
    {
      "id": "2602.16204v1",
      "title": "Linked Data Classification using Neurochaos Learning",
      "authors": [
        "Pooja Honna",
        "Ayush Patravali",
        "Nithin Nagaraj",
        "Nanjangud C. Narendra"
      ],
      "abstract": "Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T05:55:59Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16204v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16204v1"
    },
    {
      "id": "2602.16101v1",
      "title": "Axle Sensor Fusion for Online Continual Wheel Fault Detection in Wayside Railway Monitoring",
      "authors": [
        "Afonso Lourenço",
        "Francisca Osório",
        "Diogo Risca",
        "Goreti Marreiros"
      ],
      "abstract": "Reliable and cost-effective maintenance is essential for railway safety, particularly at the wheel-rail interface, which is prone to wear and failure. Predictive maintenance frameworks increasingly leverage sensor-generated time-series data, yet traditional methods require manual feature engineering, and deep learning models often degrade in online settings with evolving operational patterns. This work presents a semantic-aware, label-efficient continual learning framework for railway fault diagnostics. Accelerometer signals are encoded via a Variational AutoEncoder into latent representations capturing the normal operational structure in a fully unsupervised manner. Importantly, semantic metadata, including axle counts, wheel indexes, and strain-based deformations, is extracted via AI-driven peak detection on fiber Bragg grating sensors (resistant to electromagnetic interference) and fused with the VAE embeddings, enhancing anomaly detection under unknown operational conditions. A lightweight gradient boosting supervised classifier stabilizes anomaly scoring with minimal labels, while a replay-based continual learning strategy enables adaptation to evolving domains without catastrophic forgetting. Experiments show the model detects minor imperfections due to flats and polygonization, while adapting to evolving operational conditions, such as changes in train type, speed, load, and track profiles, captured using a single accelerometer and strain gauge in wayside monitoring.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T00:14:18Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16101v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16101v1"
    },
    {
      "id": "2602.16740v1",
      "title": "Quantifying LLM Attention-Head Stability: Implications for Circuit Universality",
      "authors": [
        "Karan Bali",
        "Jack Stanley",
        "Praneet Suresh",
        "Danilo Bzdok"
      ],
      "abstract": "In mechanistic interpretability, recent work scrutinizes transformer \"circuits\" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confidence in safety-critical settings. Here, we systematically study stability across-refits in increasingly complex transformer language models of various sizes. We quantify, layer by layer, how similarly attention heads learn representations across independently initialized training runs. Our rigorous experiments show that (1) middle-layer heads are the least stable yet the most representationally distinct; (2) deeper models exhibit stronger mid-depth divergence; (3) unstable heads in deeper layers become more functionally important than their peers from the same layer; (4) applying weight decay optimization substantially improves attention-head stability across random model initializations; and (5) the residual stream is comparatively stable. Our findings establish the cross-instance robustness of circuits as an essential yet underappreciated prerequisite for scalable oversight, drawing contours around possible white-box monitorability of AI systems.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T23:30:59Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16740v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16740v1"
    },
    {
      "id": "2602.16000v1",
      "title": "Imaging-Derived Coronary Fractional Flow Reserve: Advances in Physics-Based, Machine-Learning, and Physics-Informed Methods",
      "authors": [
        "Tanxin Zhu",
        "Emran Hossen",
        "Chen Zhao",
        "Michele Esposito",
        "Jiguang Sun",
        "Weihua Zhou"
      ],
      "abstract": "Purpose of Review Imaging derived fractional flow reserve (FFR) is rapidly evolving beyond conventional computational fluid dynamics (CFD) based pipelines toward machine learning (ML), deep learning (DL), and physics informed approaches that enable fast, wire free, and scalable functional assessment of coronary stenosis. This review synthesizes recent advances in CT and angiography based FFR, with particular emphasis on emerging physics informed neural networks and neural operators (PINNs and PINOs) and key considerations for their clinical translation. Recent Findings ML/DL approaches have markedly improved automation and computational speed, enabling prediction of pressure and FFR from anatomical descriptors or angiographic contrast dynamics. However, their real-world performance and generalizability can remain variable and sensitive to domain shift, due to multi-center heterogeneity, interpretability challenges, and differences in acquisition protocols and image quality. Physics informed learning introduces conservation structure and boundary condition consistency into model training, improving generalizability and reducing dependence on dense supervision while maintaining rapid inference. Recent evaluation trends increasingly highlight deployment oriented metrics, including calibration, uncertainty quantification, and quality control gatekeeping, as essential for safe clinical use. Summary The field is converging toward imaging derived FFR methods that are faster, more automated, and more reliable. While ML/DL offers substantial efficiency gains, physics informed frameworks such as PINNs and PINOs may provide a more robust balance between speed and physical consistency. Prospective multi center validation and standardized evaluation will be critical to support broad and safe clinical adoption.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T20:46:25Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16000v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16000v1"
    },
    {
      "id": "2602.15961v1",
      "title": "R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions",
      "authors": [
        "Zhi Sheng",
        "Yuan Yuan",
        "Guozhen Zhang",
        "Yong Li"
      ],
      "abstract": "The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T19:22:49Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15961v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15961v1"
    },
    {
      "id": "2602.15930v1",
      "title": "A targeted machine learning approach for detecting diffuse radio emission with Astronomaly: Protege",
      "authors": [
        "Verlon Etsebeth",
        "Michelle Lochner",
        "Konstantinos Kolokythas",
        "Kenda Knowles",
        "Emma Tolley"
      ],
      "abstract": "Diffuse radio emission in galaxy clusters, such as radio halos, relics, and mini halos, is a key tracer of non-thermal processes, turbulence, and magnetic fields within the intra-cluster medium. However, their low surface brightness, as well as contamination from compact sources and imaging artefacts, makes their detection challenging. The sheer volume of data from instruments such as the Square Kilometre Array will render traditional manual-inspection based detection methods infeasible. This paper introduces a novel machine learning approach that uses active learning to rapidly identify diffuse emission candidates from a small, optimally-selected subset of data. We apply the self-supervised deep learning algorithm Bootstrap Your Own Latent to extract features from source cutouts in the MeerKAT Galaxy Cluster Legacy Survey (MGCLS). We then pass these features through the Astronomaly: Protege anomaly detection framework to identify the final candidates. Using a human-labelled set, we evaluate our pipeline on high-resolution (~7''), convolved (15''), and combined-feature MGCLS datasets. Interestingly, the high-resolution features identify diffuse sources more efficiently than the convolved resolution, which are in turn outperformed by the combined features. Of the top 100 sources ranked by Protege, 99% exhibit diffuse characteristics, with 55% confirmed as cluster-related emission. Our work shows that Protege can identify diffuse emission with minimal human labelling effort, offering a powerful, scalable tool capable of detecting both known and novel diffuse radio sources.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T19:00:01Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15930v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15930v1"
    },
    {
      "id": "2602.15830v1",
      "title": "Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution",
      "authors": [
        "Christopher David Roberts"
      ],
      "abstract": "Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion. We introduce trajectory transformers as a proof-of-concept that ensemble-size independence can be achieved. This approach is an adaptation of the Post-processing Ensembles with Transformers (PoET) framework and applies self-attention over lead time while preserving the conditional independence required by aCRPS. When applied to weekly mean $T_{2m}$ forecasts from the ECMWF subseasonal forecasting system, this approach successfully reduces systematic model biases whilst also improving or maintaining forecast reliability regardless of the ensemble size used in training (3 vs 9 members) or real-time forecasts (9 vs 100 members).",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T18:59:55Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15830v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15830v1"
    }
  ]
}