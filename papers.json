{
  "last_updated": "2026-02-24T01:01:10.307376",
  "count": 20,
  "papers": [
    {
      "id": "2602.18406v1",
      "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges",
      "authors": [
        "Minh Dinh",
        "Stéphane Deny"
      ],
      "abstract": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T18:14:05Z",
      "pdf_link": "http://arxiv.org/pdf/2602.18406v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.18406v1"
    },
    {
      "id": "2602.18308v1",
      "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections",
      "authors": [
        "Biswa Sengupta",
        "Jinhua Wang",
        "Leo Brunswic"
      ],
      "abstract": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T16:06:01Z",
      "pdf_link": "http://arxiv.org/pdf/2602.18308v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.18308v1"
    },
    {
      "id": "2602.18248v1",
      "title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver",
      "authors": [
        "Pietro Sittoni",
        "Emanuele Zangrando",
        "Angelo A. Casulli",
        "Nicola Guglielmi",
        "Francesco Tudisco"
      ],
      "abstract": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T14:31:08Z",
      "pdf_link": "http://arxiv.org/pdf/2602.18248v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.18248v1"
    },
    {
      "id": "2602.18146v1",
      "title": "Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks",
      "authors": [
        "Lionel Salesses",
        "Larbi Arbaoui",
        "Tariq Benamara",
        "Arnaud Francois",
        "Caroline Sainvitu"
      ],
      "abstract": "Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T11:22:47Z",
      "pdf_link": "http://arxiv.org/pdf/2602.18146v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.18146v1"
    },
    {
      "id": "2602.18119v1",
      "title": "RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis",
      "authors": [
        "Chris Tomy",
        "Mo Vali",
        "David Pertzborn",
        "Tammam Alamatouri",
        "Anna Mühlig",
        "Orlando Guntinas-Lichius",
        "Anna Xylander",
        "Eric Michele Fantuzzi",
        "Matteo Negro",
        "Francesco Crisafi",
        "Pietro Lio",
        "Tiago Azevedo"
      ],
      "abstract": "Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T10:18:27Z",
      "pdf_link": "http://arxiv.org/pdf/2602.18119v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.18119v1"
    },
    {
      "id": "2602.18060v1",
      "title": "Deepmechanics",
      "authors": [
        "Abhay Shinde",
        "Aryan Amit Barsainyan",
        "Jose Siguenza",
        "Ankita Vaishnobi Bisoi",
        "Rakshit Kr. Singh",
        "Bharath Ramsundar"
      ],
      "abstract": "Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T08:27:43Z",
      "pdf_link": "http://arxiv.org/pdf/2602.18060v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.18060v1"
    },
    {
      "id": "2602.17978v1",
      "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees",
      "authors": [
        "Daqian Shao"
      ],
      "abstract": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T04:24:49Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17978v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17978v1"
    },
    {
      "id": "2602.17948v1",
      "title": "A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion",
      "authors": [
        "Yu Bai",
        "Zhe Wang",
        "Jiarui Zhang",
        "Dong-Xiao Zhang",
        "Yinjun Gao",
        "Jun-Jie Zhang"
      ],
      "abstract": "The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\\%$ to $95.63\\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \\emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \\emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.",
      "published": "February 20, 2026",
      "published_raw": "2026-02-20T02:58:29Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17948v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17948v1"
    },
    {
      "id": "2602.17865v1",
      "title": "Financial time series augmentation using transformer based GAN architecture",
      "authors": [
        "Andrzej Podobiński",
        "Jarosław A. Chudziak"
      ],
      "abstract": "Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T22:02:09Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17865v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17865v1"
    },
    {
      "id": "2602.17797v1",
      "title": "Deep Learning for Dermatology: An Innovative Framework for Approaching Precise Skin Cancer Detection",
      "authors": [
        "Mohammad Tahmid Noor",
        "B. M. Shahria Alam",
        "Tasmiah Rahman Orpa",
        "Shaila Afroz Anika",
        "Mahjabin Tasnim Samiha",
        "Fahad Ahammed"
      ],
      "abstract": "Skin cancer can be life-threatening if not diagnosed early, a prevalent yet preventable disease. Globally, skin cancer is perceived among the finest prevailing cancers and millions of people are diagnosed each year. For the allotment of benign and malignant skin spots, an area of critical importance in dermatological diagnostics, the application of two prominent deep learning models, VGG16 and DenseNet201 are investigated by this paper. We evaluate these CNN architectures for their efficacy in differentiating benign from malignant skin lesions leveraging enhancements in deep learning enforced to skin cancer spotting. Our objective is to assess model accuracy and computational efficiency, offering insights into how these models could assist in early detection, diagnosis, and streamlined workflows in dermatology. We used two deep learning methods DenseNet201 and VGG16 model on a binary class dataset containing 3297 images. The best result with an accuracy of 93.79% achieved by DenseNet201. All images were resized to 224x224 by rescaling. Although both models provide excellent accuracy, there is still some room for improvement. In future using new datasets, we tend to improve our work by achieving great accuracy.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T19:59:39Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17797v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17797v1"
    },
    {
      "id": "2602.17772v1",
      "title": "Sparse Bayesian Modeling of EEG Channel Interactions Improves P300 Brain-Computer Interface Performance",
      "authors": [
        "Guoxuan Ma",
        "Yuan Zhong",
        "Moyan Li",
        "Yuxiao Nie",
        "Jian Kang"
      ],
      "abstract": "Electroencephalography (EEG)-based P300 brain-computer interfaces (BCIs) enable communication without physical movement by detecting stimulus-evoked neural responses. Accurate and efficient decoding remains challenging due to high dimensionality, temporal dependence, and complex interactions across EEG channels. Most existing approaches treat channels independently or rely on black-box machine learning models, limiting interpretability and personalization. We propose a sparse Bayesian time-varying regression framework that explicitly models pairwise EEG channel interactions while performing automatic temporal feature selection. The model employs a relaxed-thresholded Gaussian process prior to induce structured sparsity in both channel-specific and interaction effects, enabling interpretable identification of task-relevant channels and channel pairs. Applied to a publicly available P300 speller dataset of 55 participants, the proposed method achieves a median character-level accuracy of 100\\% using all stimulus sequences and attains the highest overall decoding performance among competing statistical and deep learning approaches. Incorporating channel interactions yields subgroup-specific gains of up to 7\\% in character-level accuracy, particularly among participants who abstained from alcohol (up to 18\\% improvement). Importantly, the proposed method improves median BCI-Utility by approximately 10\\% at its optimal operating point, achieving peak throughput after only seven stimulus sequences. These results demonstrate that explicitly modeling structured EEG channel interactions within a principled Bayesian framework enhances predictive accuracy, improves user-centric throughput, and supports personalization in P300 BCI systems.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T19:03:51Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17772v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17772v1"
    },
    {
      "id": "2602.17642v1",
      "title": "A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning",
      "authors": [
        "Dhruv Talwar",
        "Harsh Desai",
        "Wendong Yin",
        "Goutam Mohanty",
        "Rafael Reveles"
      ],
      "abstract": "Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T18:54:06Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17642v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17642v1"
    },
    {
      "id": "2602.17747v1",
      "title": "AgriVariant: Variant Effect Prediction using DeepChem-Variant for Precision Breeding in Rice",
      "authors": [
        "Ankita Vaishnobi Bisoi",
        "Bharath Ramsundar"
      ],
      "abstract": "Predicting functional consequences of genetic variants in crop genes remains a critical bottleneck for precision breeding programs. We present AgriVariant, an end-to-end pipeline for variant-effect prediction in rice (Oryza sativa) that addresses the lack of crop-specific variant-interpretation tools and can be extended to any crop species with available reference genomes and gene annotations. Our approach integrates deep learning-based variant calling (DeepChem-Variant) with custom plant genomics annotation using RAP-DB gene models and database-independent deleteriousness scoring that combines the Grantham distance and the BLOSUM62 substitution matrix. We validate the pipeline through targeted mutations in stress-response genes (OsDREB2a, OsDREB1F, SKC1), demonstrating correct classification of stop-gained, missense, and synonymous variants with appropriate HIGH / MODERATE / LOW impact assignments. An exhaustive mutagenesis study of OsMT-3a analyzed all 1,509 possible single-nucleotide variants in 10 days, identifying 353 high-impact, 447 medium-impact, and 709 low-impact variants - an analysis that would have required 2-4 years using traditional wet-lab approaches. This computational framework enables breeders to prioritize variants for experimental validation across diverse crop species, reducing screening costs and accelerating development of climate-resilient crop varieties.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T14:03:37Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17747v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17747v1"
    },
    {
      "id": "2602.17321v1",
      "title": "The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound",
      "authors": [
        "Christoph Balada",
        "Aida Romano-Martinez",
        "Payal Varshney",
        "Vincent ten Cate",
        "Katharina Geschke",
        "Jonas Tesarz",
        "Paul Claßen",
        "Alexander K. Schuster",
        "Dativa Tibyampansha",
        "Karl-Patrik Kresoja",
        "Philipp S. Wild",
        "Sheraz Ahmed",
        "Andreas Dengel"
      ],
      "abstract": "Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T12:37:48Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17321v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17321v1"
    },
    {
      "id": "2602.17102v1",
      "title": "Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction",
      "authors": [
        "Sai Vineeth Kandappareddigari",
        "Santhoshkumar Jagadish",
        "Gauri Verma",
        "Ilhuicamina Contreras",
        "Christopher Dignam",
        "Anmol Srivastava",
        "Benjamin Demers"
      ],
      "abstract": "This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T05:59:55Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17102v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17102v1"
    },
    {
      "id": "2602.17085v1",
      "title": "ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions",
      "authors": [
        "Shogo Sato",
        "Kazuo Tanaka",
        "Shojun Ogasawara",
        "Kazuki Yamamoto",
        "Kazuhiko Murasaki",
        "Ryuichi Tanida",
        "Jun Kataoka"
      ],
      "abstract": "Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.",
      "published": "February 19, 2026",
      "published_raw": "2026-02-19T05:11:41Z",
      "pdf_link": "http://arxiv.org/pdf/2602.17085v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.17085v1"
    },
    {
      "id": "2602.16908v1",
      "title": "Multi-objective optimization and quantum hybridization of equivariant deep learning interatomic potentials on organic and inorganic compounds",
      "authors": [
        "G. Laskaris",
        "D. Morozov",
        "D. Tarpanov",
        "A. Seth",
        "J. Procelewska",
        "G. Sai Gautam",
        "A. Sagingalieva",
        "R. Brasher",
        "A. Melnikov"
      ],
      "abstract": "Allegro is a machine learning interatomic potential (MLIP) model designed to predict atomic properties in molecules using E(3) equivariant neural networks. When training this model, there tends to be a trade-off between accuracy and inference time. For this reason we apply multi-objective hyperparameter optimization to the two objectives. Additionally, we experiment with modified architectures by making variants of Allegro some by adding strictly classical multi-layer perceptron (MLP) layers and some by adding quantum-classical hybrid layers. We compare the results from QM9, rMD17-aspirin, rMD17-benzene and our own proprietary dataset consisting of copper and lithium atoms. As results, we have a list of variants that surpass the Allegro in accuracy and also results which demonstrate the trade-off with inference times.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T21:48:35Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16908v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16908v1"
    },
    {
      "id": "2602.16696v1",
      "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks",
      "authors": [
        "Huan Souza",
        "Pankaj Mehta"
      ],
      "abstract": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T18:42:29Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16696v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16696v1"
    },
    {
      "id": "2602.16548v1",
      "title": "RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion",
      "authors": [
        "Tianmeng Hu",
        "Yongzheng Cui",
        "Biao Luo",
        "Ke Li"
      ],
      "abstract": "The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T15:52:26Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16548v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16548v1"
    },
    {
      "id": "2602.16507v1",
      "title": "Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?",
      "authors": [
        "Gaetan De Waele",
        "Marek Wydmuch",
        "Krzysztof Dembczyński",
        "Wojciech Kotłowski",
        "Willem Waegeman"
      ],
      "abstract": "One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.",
      "published": "February 18, 2026",
      "published_raw": "2026-02-18T14:48:08Z",
      "pdf_link": "http://arxiv.org/pdf/2602.16507v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.16507v1"
    }
  ]
}