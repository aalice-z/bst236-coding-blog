{
  "last_updated": "2026-02-17T16:08:12.869622",
  "count": 20,
  "papers": [
    {
      "id": "2602.15021v1",
      "title": "Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI",
      "authors": [
        "Xiaosheng Zhao",
        "Yuan-Sen Ting",
        "Rosemary F. G. Wyse",
        "Alexander S. Szalay",
        "Yang Huang",
        "László Dobos",
        "Tamás Budavári",
        "Viska Wei"
      ],
      "abstract": "Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] > -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T18:58:47Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15021v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15021v1"
    },
    {
      "id": "2602.14919v1",
      "title": "BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs",
      "authors": [
        "Tianyi Ma",
        "Yiyue Qian",
        "Zehong Wang",
        "Zheyuan Zhang",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T16:55:37Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14919v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14919v1"
    },
    {
      "id": "2602.14803v1",
      "title": "A physics inspired and efficient transform for optoacoustic systems",
      "authors": [
        "Maria Rodriguez Saenz de Tejada",
        "Alvaro Jimenez",
        "Rodrigo Rojo",
        "Sergio Contador",
        "Juan Aguirre"
      ],
      "abstract": "Optoacoustic imaging technologies still rely on the Discrete Fourier Transform (DFT) as the preferred method for essential signal conditioning operations. The long-standing success of the DFT is largely due to the limited generalizability of data-driven alternatives (e.g., deep neural networks) and the inconsistent accuracy of other analytical transforms (e.g., wavelet transforms). However, the DFT is a 'general-purpose' algorithm whose building blocks, when considered individually, are not directly related to the physics of optoacoustic signal generation. As a result, its accuracy can be limited. Moreover, the computational complexity of its modern algorithmic form imposes a hard limit on execution speed. This limitation may be particularly pronounced in advanced spectral systems that produce large data sets, preventing their adoption. Importantly, optoacoustic imaging technologies are increasingly impacting biological research while standing on the brink of widespread clinical adoption. We have devised a new transform whose building blocks are directly inspired by the physics of optoacoustic signal generation. We have compared its performance with the DFT and other classical transforms on typical signal processing tasks using both simulations and experimental data sets. Our results indicate that the proposed transform not only establishes a new lower bound in computational complexity relative to the DFT, but also strongly outperforms the classical transforms for basic signal processing operations in terms of accuracy metrics. We expect our new transform to catalyze the general adoption of optoacoustic methods.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T14:54:00Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14803v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14803v1"
    },
    {
      "id": "2602.14663v1",
      "title": "Pseudo-differential-enhanced physics-informed neural networks",
      "authors": [
        "Andrew Gracyk"
      ],
      "abstract": "We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T11:40:58Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14663v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14663v1"
    },
    {
      "id": "2602.14523v1",
      "title": "Architectural Insights for Post-Tornado Damage Recognition",
      "authors": [
        "Robinson Umeike",
        "Thang Dao",
        "Shane Crawford",
        "John van de Lindt",
        "Blythe Johnston",
        " Wanting",
        " Wang",
        "Trung Do",
        "Ajibola Mofikoya",
        "Sarbesh Banjara",
        "Cuong Pham"
      ],
      "abstract": "Rapid and accurate building damage assessment in the immediate aftermath of tornadoes is critical for coordinating life-saving search and rescue operations, optimizing emergency resource allocation, and accelerating community recovery. However, current automated methods struggle with the unique visual complexity of tornado-induced wreckage, primarily due to severe domain shift from standard pre-training datasets and extreme class imbalance in real-world disaster data. To address these challenges, we introduce a systematic experimental framework evaluating 79 open-source deep learning models, encompassing both Convolutional Neural Networks (CNNs) and Vision Transformers, across over 2,300 controlled experiments on our newly curated Quad-State Tornado Damage (QSTD) benchmark dataset. Our findings reveal that achieving operational-grade performance hinges on a complex interaction between architecture and optimization, rather than architectural selection alone. Most strikingly, we demonstrate that optimizer choice can be more consequential than architecture: switching from Adam to SGD provided dramatic F1 gains of +25 to +38 points for Vision Transformer and Swin Transformer families, fundamentally reversing their ranking from bottom-tier to competitive with top-performing CNNs. Furthermore, a low learning rate of 1x10^(-4) proved universally critical, boosting average F1 performance by +10.2 points across all architectures. Our champion model, ConvNeXt-Base trained with these optimized settings, demonstrated strong cross-event generalization on the held-out Tuscaloosa-Moore Tornado Damage (TMTD) dataset, achieving 46.4% Macro F1 (+34.6 points over baseline) and retaining 85.5% Ordinal Top-1 Accuracy despite temporal and sensor domain shifts.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T07:16:33Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14523v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14523v1"
    },
    {
      "id": "2602.14486v1",
      "title": "Revisiting the Platonic Representation Hypothesis: An Aristotelian View",
      "authors": [
        "Fabian Gröger",
        "Shuo Wen",
        "Maria Brbić"
      ],
      "abstract": "The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T06:01:23Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14486v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14486v1"
    },
    {
      "id": "2602.14100v1",
      "title": "Character-aware Transformers Learn an Irregular Morphological Pattern Yet None Generalize Like Humans",
      "authors": [
        "Akhilesh Kakolu Ramarao",
        "Kevin Tang",
        "Dinah Baer-Henney"
      ],
      "abstract": "Whether neural networks can serve as cognitive models of morphological learning remains an open question. Recent work has shown that encoder-decoder models can acquire irregular patterns, but evidence that they generalize these patterns like humans is mixed. We investigate this using the Spanish \\emph{L-shaped morphome}, where only the first-person singular indicative (e.g., \\textit{pongo} `I put') shares its stem with all subjunctive forms (e.g., \\textit{ponga, pongas}) despite lacking apparent phonological, semantic, or syntactic motivation. We compare five encoder-decoder transformers varying along two dimensions: sequential vs. position-invariant positional encoding, and atomic vs. decomposed tag representations. Positional encoding proves decisive: position-invariant models recover the correct L-shaped paradigm clustering even when L-shaped verbs are scarce in training, whereas sequential positional encoding models only partially capture the pattern. Yet none of the models productively generalize this pattern to novel forms. Position-invariant models generalize the L-shaped stem across subjunctive cells but fail to extend it to the first-person singular indicative, producing a mood-based generalization rather than the L-shaped morphomic pattern. Humans do the opposite, generalizing preferentially to the first-person singular indicative over subjunctive forms. None of the models reproduce the human pattern, highlighting the gap between statistical pattern reproduction and morphological abstraction.",
      "published": "February 15, 2026",
      "published_raw": "2026-02-15T11:22:12Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14100v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14100v1"
    },
    {
      "id": "2602.14094v1",
      "title": "Wireless Physical Neural Networks (WPNNs): Opportunities and Challenges",
      "authors": [
        "Meng Hua",
        "Itsik Bergel",
        "Tolga Girici",
        "Marco Di Renzo",
        "Deniz Gunduz"
      ],
      "abstract": "Wireless communication systems exhibit structural and functional similarities to neural networks: signals propagate through cascaded elements, interact with the environment, and undergo transformations. Building upon this perspective, we introduce a unified paradigm, termed \\textit{wireless physical neural networks (WPNNs)}, in which components of a wireless network, such as transceivers, relays, backscatter, and intelligent surfaces, are interpreted as computational layers within a learning architecture. By treating the wireless propagation environment and network elements as differentiable operators, new opportunities arise for joint communication-computation designs, where system optimization can be achieved through learning-based methods applied directly to the physical network. This approach may operate independently of, or in conjunction with, conventional digital neural layers, enabling hybrid communication learning pipelines. In the article, we outline representative architectures that embody this viewpoint and discuss the algorithmic and training considerations required to leverage the wireless medium as a computational resource. Through numerical examples, we highlight the potential performance gains in processing, adaptability, efficiency, and end-to-end optimization, demonstrating the promise of reconfiguring wireless systems as learning networks in next-generation communication frameworks.",
      "published": "February 15, 2026",
      "published_raw": "2026-02-15T10:58:44Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14094v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14094v1"
    },
    {
      "id": "2602.13690v1",
      "title": "Physics Aware Neural Networks: Denoising for Magnetic Navigation",
      "authors": [
        "Aritra Das",
        "Yashas Shende",
        "Muskaan Chugh",
        "Reva Laxmi Chauhan",
        "Arghya Pathak",
        "Debayan Gupta"
      ],
      "abstract": "Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T09:23:57Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13690v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13690v1"
    },
    {
      "id": "2602.13550v1",
      "title": "Out-of-Support Generalisation via Weight Space Sequence Modelling",
      "authors": [
        "Roussel Desmond Nzoyem"
      ],
      "abstract": "As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T01:51:54Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13550v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13550v1"
    },
    {
      "id": "2602.13536v1",
      "title": "Robustness Verification of Binary Neural Networks: An Ising and Quantum-Inspired Framework",
      "authors": [
        "Rahul Singh",
        "Seyran Saeedi",
        "Zheng Zhang"
      ],
      "abstract": "Binary neural networks (BNNs) are increasingly deployed in edge computing applications due to their low hardware complexity and high energy efficiency. However, verifying the robustness of BNNs against input perturbations, including adversarial attacks, remains computationally challenging because the underlying decision problem is inherently combinatorial. In this paper, we propose an Ising- and quantum-inspired framework for BNN robustness verification. We show that, for a broad class of BNN architectures, robustness verification can be formulated as a Quadratic Constrained Boolean Optimization (QCBO) problem and subsequently transformed into a Quadratic Unconstrained Boolean Optimization (QUBO) instance amenable to Ising and quantum-inspired solvers. We demonstrate the feasibility of this formulation on binarized MNIST by solving the resulting QUBOs with a free energy machine (FEM) solver and simulated annealing. We also show the deployment of this framework on quantum annealing and digital annealing platforms. Our results highlight the potential of quantum-inspired computing and Ising computing as a pathway toward trustworthy AI systems.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T00:28:51Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13536v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13536v1"
    },
    {
      "id": "2602.12998v1",
      "title": "Variational study of the magnetization plateaus in the spin-1/2 kagome Heisenberg antiferromagnet: an approach from vision transformer neural quantum states",
      "authors": [
        "Andreas Raikos",
        "Sylvain Capponi",
        "Fabien Alet"
      ],
      "abstract": "We analyze the magnetization curve of the spin-1/2 kagome Heisenberg model in a magnetic field. Using state-of-the-art variational wavefunctions based on neural networks, we confirm the presence of robust magnetization plateaus at $m=1/3$, $5/9$ and $7/9$ of the saturation value, stabilized by a spontaneous symmetry breaking of lattice translations with a $\\sqrt{3}\\times \\sqrt{3}$ unit cell. Regarding the more challenging $m=1/9$ plateau, we find two competing valence bond crystals depending on the system size, both breaking translation as well as point group symmetries and with a larger $3\\times 3$ unit cell. Such quantum states with local modulations of the magnetization average values could be observed experimentally in the near future.",
      "published": "February 13, 2026",
      "published_raw": "2026-02-13T15:09:03Z",
      "pdf_link": "http://arxiv.org/pdf/2602.12998v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.12998v1"
    },
    {
      "id": "2602.12798v1",
      "title": "Can Neural Networks Provide Latent Embeddings for Telemetry-Aware Greedy Routing?",
      "authors": [
        "Andreas Boltres",
        "Niklas Freymuth",
        "Gerhard Neumann"
      ],
      "abstract": "Telemetry-Aware routing promises to increase efficacy and responsiveness to traffic surges in computer networks. Recent research leverages Machine Learning to deal with the complex dependency between network state and routing, but sacrifices explainability of routing decisions due to the black-box nature of the proposed neural routing modules. We propose \\emph{Placer}, a novel algorithm using Message Passing Networks to transform network states into latent node embeddings. These embeddings facilitate quick greedy next-hop routing without directly solving the all-pairs shortest paths problem, and let us visualize how certain network events shape routing decisions.",
      "published": "February 13, 2026",
      "published_raw": "2026-02-13T10:31:09Z",
      "pdf_link": "http://arxiv.org/pdf/2602.12798v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.12798v1"
    },
    {
      "id": "2602.12595v1",
      "title": "Michel Talagrand and the Rigorous Theory of Mean Field Spin Glasses",
      "authors": [
        "Sourav Chatterjee"
      ],
      "abstract": "Michel Talagrand played a decisive role in the transformation of mean field spin glass theory into a rigorous mathematical subject. This chapter offers a narrative account of that development. We begin with the physical origins of the Sherrington-Kirkpatrick (SK) model and the emergence of the TAP and Almeida-Thouless stability frameworks, culminating in Parisi's replica symmetry breaking (RSB) ansatz and its hierarchical order parameter. We then review early rigorous milestones, including high-temperature results and stability identities, and describe the consolidation of interpolation and cavity methods through the work of Guerra and of Aizenman-Sims-Starr. The central event in this narrative is Talagrand's 2006 proof of the Parisi formula for the SK model and for a broad class of mixed $p$-spin models, and his subsequent analysis of Parisi measures. We also discuss Talagrand's later program constructing pure states under extended Ghirlanda-Guerra identities and an atom at the maximal overlap, together with the structural results that followed, notably Panchenko's ultrametricity theorem and extensions of the Parisi formula. Throughout, we indicate how related contributions by many authors fit into the same long-running program across probability, analysis, and mathematical physics.",
      "published": "February 13, 2026",
      "published_raw": "2026-02-13T04:13:30Z",
      "pdf_link": "http://arxiv.org/pdf/2602.12595v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.12595v1"
    },
    {
      "id": "2602.12390v1",
      "title": "Rational Neural Networks have Expressivity Advantages",
      "authors": [
        "Maosen Tang",
        "Alex Townsend"
      ],
      "abstract": "We study neural networks with trainable low-degree rational activation functions and show that they are more expressive and parameter-efficient than modern piecewise-linear and smooth activations such as ELU, LeakyReLU, LogSigmoid, PReLU, ReLU, SELU, CELU, Sigmoid, SiLU, Mish, Softplus, Tanh, Softmin, Softmax, and LogSoftmax. For an error target of $\\varepsilon>0$, we establish approximation-theoretic separations: Any network built from standard fixed activations can be uniformly approximated on compact domains by a rational-activation network with only $\\mathrm{poly}(\\log\\log(1/\\varepsilon))$ overhead in size, while the converse provably requires $Ω(\\log(1/\\varepsilon))$ parameters in the worst case. This exponential gap persists at the level of full networks and extends to gated activations and transformer-style nonlinearities. In practice, rational activations integrate seamlessly into standard architectures and training pipelines, allowing rationals to match or outperform fixed activations under identical architectures and optimizers.",
      "published": "February 12, 2026",
      "published_raw": "2026-02-12T20:33:42Z",
      "pdf_link": "http://arxiv.org/pdf/2602.12390v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.12390v1"
    },
    {
      "id": "2602.12255v1",
      "title": "Vision Transformer for Multi-Domain Phase Retrieval in Coherent Diffraction Imaging",
      "authors": [
        "Jialun Liu",
        "David Yang",
        "Ian Robinson"
      ],
      "abstract": "Bragg coherent diffraction imaging (BCDI) phase retrieval becomes rapidly difficult in the strong-phase regime, where a crystal contains distortions beyond half a lattice spacing. An important special case is the phase domain problem, where blocks of a crystal are displaced with sharp jumps at domain walls. The strong-phase, here defined as beyond $\\pm π/2$, generates split Bragg peaks and dense fringe structure for which classical iterative solvers often stagnate or return different solutions from different initialisations. Here, we introduce an unsupervised Fourier Vision Transformer (Fourier ViT) to solve this block-phase, multi-domain phase-retrieval problem directly from measured 2D Bragg diffraction intensities. Fourier ViT couples reciprocal-space information globally through multiscale Fourier token mixing, while shallow convolutional front and back-ends provide local filtering and reconstruction. We validate the approach on large-scale synthetic datasets of Voronoi multi-domain crystals with strong-phase contrast under realistic noise corruptions, and on experimental diffraction from a $\\mathrm{La}_{2-x}\\mathrm{Ca}_x\\mathrm{MnO}_4$ nanocrystal. Across the regimes considered, Fourier ViT achieves the lowest reciprocal-space mismatch ($χ^2$) among the compared methods and preserves domain-resolved phase reconstructions for increasing numbers of domains. On experimental data, with the same real-space support, Fourier ViT matches the iterative benchmark $χ^2$ while improving robustness to random initialisations, yielding a higher success rate of low-$χ^2$ reconstructions than the complex convolutional neural network baseline.",
      "published": "February 12, 2026",
      "published_raw": "2026-02-12T18:42:21Z",
      "pdf_link": "http://arxiv.org/pdf/2602.12255v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.12255v1"
    },
    {
      "id": "2602.12251v1",
      "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
      "authors": [
        "Ralph Krüger"
      ],
      "abstract": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.",
      "published": "February 12, 2026",
      "published_raw": "2026-02-12T18:37:23Z",
      "pdf_link": "http://arxiv.org/pdf/2602.12251v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.12251v1"
    },
    {
      "id": "2602.11974v1",
      "title": "Measurement of the singly Cabibbo-suppressed decay $Λ_c^+\\to pη'$ with Deep Learning",
      "authors": [
        " BESIII Collaboration",
        "M. Ablikim",
        "M. N. Achasov",
        "P. Adlarson",
        "X. C. Ai",
        "R. Aliberti",
        "A. Amoroso",
        "Q. An",
        "Y. Bai",
        "O. Bakina",
        "Y. Ban",
        "H. -R. Bao",
        "X. L. Bao",
        "V. Batozskaya",
        "K. Begzsuren",
        "N. Berger",
        "M. Berlowski",
        "M. B. Bertani",
        "D. Bettoni",
        "F. Bianchi",
        "E. Bianco",
        "A. Bortone",
        "I. Boyko",
        "R. A. Briere",
        "A. Brueggemann",
        "H. Cai",
        "M. H. Cai",
        "X. Cai",
        "A. Calcaterra",
        "G. F. Cao",
        "N. Cao",
        "S. A. Cetin",
        "X. Y. Chai",
        "J. F. Chang",
        "T. T. Chang",
        "G. R. Che",
        "Y. Z. Che",
        "C. H. Chen",
        "Chao Chen",
        "G. Chen",
        "H. S. Chen",
        "H. Y. Chen",
        "M. L. Chen",
        "S. J. Chen",
        "S. M. Chen",
        "T. Chen",
        "W. Chen",
        "X. R. Chen",
        "X. T. Chen",
        "X. Y. Chen",
        "Y. B. Chen",
        "Y. Q. Chen",
        "Z. K. Chen",
        "J. Cheng",
        "L. N. Cheng",
        "S. K. Choi",
        "X. Chu",
        "G. Cibinetto",
        "F. Cossio",
        "J. Cottee-Meldrum",
        "H. L. Dai",
        "J. P. Dai",
        "X. C. Dai",
        "A. Dbeyssi",
        "R. E. de Boer",
        "D. Dedovich",
        "C. Q. Deng",
        "Z. Y. Deng",
        "A. Denig",
        "I. Denisenko",
        "M. Destefanis",
        "F. De Mori",
        "X. X. Ding",
        "Y. Ding",
        "Y. X. Ding",
        "J. Dong",
        "L. Y. Dong",
        "M. Y. Dong",
        "X. Dong",
        "M. C. Du",
        "S. X. Du",
        "S. X. Du",
        "X. L. Du",
        "Y. Y. Duan",
        "Z. H. Duan",
        "P. Egorov",
        "G. F. Fan",
        "J. J. Fan",
        "Y. H. Fan",
        "J. Fang",
        "J. Fang",
        "S. S. Fang",
        "W. X. Fang",
        "Y. Q. Fang",
        "L. Fava",
        "F. Feldbauer",
        "G. Felici",
        "C. Q. Feng",
        "J. H. Feng",
        "L. Feng",
        "Q. X. Feng",
        "Y. T. Feng",
        "M. Fritsch",
        "C. D. Fu",
        "J. L. Fu",
        "Y. W. Fu",
        "H. Gao",
        "Y. Gao",
        "Y. N. Gao",
        "Y. N. Gao",
        "Y. Y. Gao",
        "Z. Gao",
        "S. Garbolino",
        "I. Garzia",
        "L. Ge",
        "P. T. Ge",
        "Z. W. Ge",
        "C. Geng",
        "E. M. Gersabeck",
        "A. Gilman",
        "K. Goetzen",
        "J. Gollub",
        "J. D. Gong",
        "L. Gong",
        "W. X. Gong",
        "W. Gradl",
        "S. Gramigna",
        "M. Greco",
        "M. D. Gu",
        "M. H. Gu",
        "C. Y. Guan",
        "A. Q. Guo",
        "J. N. Guo",
        "L. B. Guo",
        "M. J. Guo",
        "R. P. Guo",
        "X. Guo",
        "Y. P. Guo",
        "A. Guskov",
        "J. Gutierrez",
        "T. T. Han",
        "F. Hanisch",
        "K. D. Hao",
        "X. Q. Hao",
        "F. A. Harris",
        "C. Z. He",
        "K. L. He",
        "F. H. Heinsius",
        "C. H. Heinz",
        "Y. K. Heng",
        "C. Herold",
        "P. C. Hong",
        "G. Y. Hou",
        "X. T. Hou",
        "Y. R. Hou",
        "Z. L. Hou",
        "H. M. Hu",
        "J. F. Hu",
        "Q. P. Hu",
        "S. L. Hu",
        "T. Hu",
        "Y. Hu",
        "Z. M. Hu",
        "G. S. Huang",
        "K. X. Huang",
        "L. Q. Huang",
        "P. Huang",
        "X. T. Huang",
        "Y. P. Huang",
        "Y. S. Huang",
        "T. Hussain",
        "N. Hüsken",
        "N. in der Wiesche",
        "J. Jackson",
        "Q. Ji",
        "Q. P. Ji",
        "W. Ji",
        "X. B. Ji",
        "X. L. Ji",
        "X. Q. Jia",
        "Z. K. Jia",
        "D. Jiang",
        "H. B. Jiang",
        "P. C. Jiang",
        "S. J. Jiang",
        "X. S. Jiang",
        "Y. Jiang",
        "J. B. Jiao",
        "J. K. Jiao",
        "Z. Jiao",
        "L. C. L. Jin",
        "S. Jin",
        "Y. Jin",
        "M. Q. Jing",
        "X. M. Jing",
        "T. Johansson",
        "S. Kabana",
        "X. L. Kang",
        "X. S. Kang",
        "B. C. Ke",
        "V. Khachatryan",
        "A. Khoukaz",
        "O. B. Kolcu",
        "B. Kopf",
        "L. Kröger",
        "M. Kuessner",
        "X. Kui",
        "N. Kumar",
        "A. Kupsc",
        "W. Kühn",
        "Q. Lan",
        "W. N. Lan",
        "T. T. Lei",
        "M. Lellmann",
        "T. Lenz",
        "C. Li",
        "C. Li",
        "C. H. Li",
        "C. K. Li",
        "D. M. Li",
        "F. Li",
        "G. Li",
        "H. B. Li",
        "H. J. Li",
        "H. L. Li",
        "H. N. Li",
        "Hui Li",
        "J. R. Li",
        "J. S. Li",
        "J. W. Li",
        "K. Li",
        "K. L. Li",
        "L. J. Li",
        "Lei Li",
        "M. H. Li",
        "M. R. Li",
        "P. L. Li",
        "P. R. Li",
        "Q. M. Li",
        "Q. X. Li",
        "R. Li",
        "S. X. Li",
        "Shanshan Li",
        "T. Li",
        "T. Y. Li",
        "W. D. Li",
        "W. G. Li",
        "X. Li",
        "X. H. Li",
        "X. K. Li",
        "X. L. Li",
        "X. Y. Li",
        "X. Z. Li",
        "Y. Li",
        "Y. G. Li",
        "Y. P. Li",
        "Z. H. Li",
        "Z. J. Li",
        "Z. X. Li",
        "Z. Y. Li",
        "C. Liang",
        "H. Liang",
        "Y. F. Liang",
        "Y. T. Liang",
        "G. R. Liao",
        "L. B. Liao",
        "M. H. Liao",
        "Y. P. Liao",
        "J. Libby",
        "A. Limphirat",
        "D. X. Lin",
        "L. Q. Lin",
        "T. Lin",
        "B. J. Liu",
        "B. X. Liu",
        "C. X. Liu",
        "F. Liu",
        "F. H. Liu",
        "Feng Liu",
        "G. M. Liu",
        "H. Liu",
        "H. B. Liu",
        "H. M. Liu",
        "Huihui Liu",
        "J. B. Liu",
        "J. J. Liu",
        "K. Liu",
        "K. Liu",
        "K. Y. Liu",
        "Ke Liu",
        "L. Liu",
        "L. C. Liu",
        "Lu Liu",
        "M. H. Liu",
        "P. L. Liu",
        "Q. Liu",
        "S. B. Liu",
        "W. M. Liu",
        "W. T. Liu",
        "X. Liu",
        "X. K. Liu",
        "X. L. Liu",
        "X. Y. Liu",
        "Y. Liu",
        "Y. Liu",
        "Y. B. Liu",
        "Z. A. Liu",
        "Z. D. Liu",
        "Z. Q. Liu",
        "Z. Y. Liu",
        "X. C. Lou",
        "H. J. Lu",
        "J. G. Lu",
        "X. L. Lu",
        "Y. Lu",
        "Y. H. Lu",
        "Y. P. Lu",
        "Z. H. Lu",
        "C. L. Luo",
        "J. R. Luo",
        "J. S. Luo",
        "M. X. Luo",
        "T. Luo",
        "X. L. Luo",
        "Z. Y. Lv",
        "X. R. Lyu",
        "Y. F. Lyu",
        "Y. H. Lyu",
        "F. C. Ma",
        "H. L. Ma",
        "Heng Ma",
        "J. L. Ma",
        "L. L. Ma",
        "L. R. Ma",
        "Q. M. Ma",
        "R. Q. Ma",
        "R. Y. Ma",
        "T. Ma",
        "X. T. Ma",
        "X. Y. Ma",
        "Y. M. Ma",
        "F. E. Maas",
        "I. MacKay",
        "M. Maggiora",
        "S. Malde",
        "Q. A. Malik",
        "H. X. Mao",
        "Y. J. Mao",
        "Z. P. Mao",
        "S. Marcello",
        "A. Marshall",
        "F. M. Melendi",
        "Y. H. Meng",
        "Z. X. Meng",
        "G. Mezzadri",
        "H. Miao",
        "T. J. Min",
        "R. E. Mitchell",
        "X. H. Mo",
        "B. Moses",
        "N. Yu. Muchnoi",
        "J. Muskalla",
        "Y. Nefedov",
        "F. Nerling",
        "H. Neuwirth",
        "Z. Ning",
        "S. Nisar",
        "Q. L. Niu",
        "W. D. Niu",
        "Y. Niu",
        "C. Normand",
        "S. L. Olsen",
        "Q. Ouyang",
        "S. Pacetti",
        "X. Pan",
        "Y. Pan",
        "A. Pathak",
        "Y. P. Pei",
        "M. Pelizaeus",
        "H. P. Peng",
        "X. J. Peng",
        "Y. Y. Peng",
        "K. Peters",
        "K. Petridis",
        "J. L. Ping",
        "R. G. Ping",
        "S. Plura",
        "V. Prasad",
        "F. Z. Qi",
        "H. R. Qi",
        "M. Qi",
        "S. Qian",
        "W. B. Qian",
        "C. F. Qiao",
        "J. H. Qiao",
        "J. J. Qin",
        "J. L. Qin",
        "L. Q. Qin",
        "L. Y. Qin",
        "P. B. Qin",
        "X. P. Qin",
        "X. S. Qin",
        "Z. H. Qin",
        "J. F. Qiu",
        "Z. H. Qu",
        "J. Rademacker",
        "C. F. Redmer",
        "A. Rivetti",
        "M. Rolo",
        "G. Rong",
        "S. S. Rong",
        "F. Rosini",
        "Ch. Rosner",
        "M. Q. Ruan",
        "N. Salone",
        "A. Sarantsev",
        "Y. Schelhaas",
        "K. Schoenning",
        "M. Scodeggio",
        "W. Shan",
        "X. Y. Shan",
        "Z. J. Shang",
        "J. F. Shangguan",
        "L. G. Shao",
        "M. Shao",
        "C. P. Shen",
        "H. F. Shen",
        "W. H. Shen",
        "X. Y. Shen",
        "B. A. Shi",
        "H. Shi",
        "J. L. Shi",
        "J. Y. Shi",
        "S. Y. Shi",
        "X. Shi",
        "H. L. Song",
        "J. J. Song",
        "M. H. Song",
        "T. Z. Song",
        "W. M. Song",
        "Y. X. Song",
        "Zirong Song",
        "S. Sosio",
        "S. Spataro",
        "S. Stansilaus",
        "F. Stieler",
        "M. Stolte",
        "S. S Su",
        "G. B. Sun",
        "G. X. Sun",
        "H. Sun",
        "H. K. Sun",
        "J. F. Sun",
        "K. Sun",
        "L. Sun",
        "R. Sun",
        "S. S. Sun",
        "T. Sun",
        "W. Y. Sun",
        "Y. C. Sun",
        "Y. H. Sun",
        "Y. J. Sun",
        "Y. Z. Sun",
        "Z. Q. Sun",
        "Z. T. Sun",
        "C. J. Tang",
        "G. Y. Tang",
        "J. Tang",
        "J. J. Tang",
        "L. F. Tang",
        "Y. A. Tang",
        "L. Y. Tao",
        "M. Tat",
        "J. X. Teng",
        "J. Y. Tian",
        "W. H. Tian",
        "Y. Tian",
        "Z. F. Tian",
        "I. Uman",
        "E. van der Smagt",
        "B. Wang",
        "B. Wang",
        "Bo Wang",
        "C. Wang",
        "C. Wang",
        "Cong Wang",
        "D. Y. Wang",
        "H. J. Wang",
        "H. R. Wang",
        "J. Wang",
        "J. J. Wang",
        "J. P. Wang",
        "K. Wang",
        "L. L. Wang",
        "L. W. Wang",
        "M. Wang",
        "M. Wang",
        "N. Y. Wang",
        "S. Wang",
        "Shun Wang",
        "T. Wang",
        "T. J. Wang",
        "W. Wang",
        "W. P. Wang",
        "X. Wang",
        "X. F. Wang",
        "X. L. Wang",
        "X. N. Wang",
        "Xin Wang",
        "Y. Wang",
        "Y. D. Wang",
        "Y. F. Wang",
        "Y. H. Wang",
        "Y. J. Wang",
        "Y. L. Wang",
        "Y. N. Wang",
        "Y. N. Wang",
        "Yaqian Wang",
        "Yi Wang",
        "Yuan Wang",
        "Z. Wang",
        "Z. Wang",
        "Z. L. Wang",
        "Z. Q. Wang",
        "Z. Y. Wang",
        "Ziyi Wang",
        "D. Wei",
        "D. H. Wei",
        "H. R. Wei",
        "F. Weidner",
        "S. P. Wen",
        "U. Wiedner",
        "G. Wilkinson",
        "M. Wolke",
        "J. F. Wu",
        "L. H. Wu",
        "L. J. Wu",
        "Lianjie Wu",
        "S. G. Wu",
        "S. M. Wu",
        "X. W. Wu",
        "Y. J. Wu",
        "Z. Wu",
        "L. Xia",
        "B. H. Xiang",
        "D. Xiao",
        "G. Y. Xiao",
        "H. Xiao",
        "Y. L. Xiao",
        "Z. J. Xiao",
        "C. Xie",
        "K. J. Xie",
        "Y. Xie",
        "Y. G. Xie",
        "Y. H. Xie",
        "Z. P. Xie",
        "T. Y. Xing",
        "D. B. Xiong",
        "C. J. Xu",
        "G. F. Xu",
        "H. Y. Xu",
        "M. Xu",
        "Q. J. Xu",
        "Q. N. Xu",
        "T. D. Xu",
        "X. P. Xu",
        "Y. Xu",
        "Y. C. Xu",
        "Z. S. Xu",
        "F. Yan",
        "L. Yan",
        "W. B. Yan",
        "W. C. Yan",
        "W. H. Yan",
        "W. P. Yan",
        "X. Q. Yan",
        "Y. Y. Yan",
        "H. J. Yang",
        "H. L. Yang",
        "H. X. Yang",
        "J. H. Yang",
        "R. J. Yang",
        "Y. Yang",
        "Y. H. Yang",
        "Y. Q. Yang",
        "Y. Z. Yang",
        "Z. P. Yao",
        "M. Ye",
        "M. H. Ye",
        "Z. J. Ye",
        "Junhao Yin",
        "Z. Y. You",
        "B. X. Yu",
        "C. X. Yu",
        "G. Yu",
        "J. S. Yu",
        "L. W. Yu",
        "T. Yu",
        "X. D. Yu",
        "Y. C. Yu",
        "Y. C. Yu",
        "C. Z. Yuan",
        "H. Yuan",
        "J. Yuan",
        "J. Yuan",
        "L. Yuan",
        "M. K. Yuan",
        "S. H. Yuan",
        "Y. Yuan",
        "C. X. Yue",
        "Ying Yue",
        "A. A. Zafar",
        "F. R. Zeng",
        "S. H. Zeng",
        "X. Zeng",
        "Y. J. Zeng",
        "Y. J. Zeng",
        "Y. C. Zhai",
        "Y. H. Zhan",
        "S. N. Zhang",
        "B. L. Zhang",
        "B. X. Zhang",
        "D. H. Zhang",
        "G. Y. Zhang",
        "G. Y. Zhang",
        "H. Zhang",
        "H. Zhang",
        "H. C. Zhang",
        "H. H. Zhang",
        "H. Q. Zhang",
        "H. R. Zhang",
        "H. Y. Zhang",
        "J. Zhang",
        "J. J. Zhang",
        "J. L. Zhang",
        "J. Q. Zhang",
        "J. S. Zhang",
        "J. W. Zhang",
        "J. X. Zhang",
        "J. Y. Zhang",
        "J. Z. Zhang",
        "Jianyu Zhang",
        "L. M. Zhang",
        "Lei Zhang",
        "N. Zhang",
        "P. Zhang",
        "Q. Zhang",
        "Q. Y. Zhang",
        "R. Y. Zhang",
        "S. H. Zhang",
        "Shulei Zhang",
        "X. M. Zhang",
        "X. Y. Zhang",
        "Y. Zhang",
        "Y. Zhang",
        "Y. T. Zhang",
        "Y. H. Zhang",
        "Y. P. Zhang",
        "Z. D. Zhang",
        "Z. H. Zhang",
        "Z. L. Zhang",
        "Z. L. Zhang",
        "Z. X. Zhang",
        "Z. Y. Zhang",
        "Z. Y. Zhang",
        "Z. Y. Zhang",
        "Zh. Zh. Zhang",
        "G. Zhao",
        "J. Y. Zhao",
        "J. Z. Zhao",
        "L. Zhao",
        "L. Zhao",
        "M. G. Zhao",
        "S. J. Zhao",
        "Y. B. Zhao",
        "Y. L. Zhao",
        "Y. P. Zhao",
        "Y. X. Zhao",
        "Z. G. Zhao",
        "A. Zhemchugov",
        "B. Zheng",
        "B. M. Zheng",
        "J. P. Zheng",
        "W. J. Zheng",
        "X. R. Zheng",
        "Y. H. Zheng",
        "B. Zhong",
        "C. Zhong",
        "H. Zhou",
        "J. Q. Zhou",
        "S. Zhou",
        "X. Zhou",
        "X. K. Zhou",
        "X. R. Zhou",
        "X. Y. Zhou",
        "Y. X. Zhou",
        "Y. Z. Zhou",
        "A. N. Zhu",
        "J. Zhu",
        "K. Zhu",
        "K. J. Zhu",
        "K. S. Zhu",
        "L. X. Zhu",
        "Lin Zhu",
        "S. H. Zhu",
        "T. J. Zhu",
        "W. D. Zhu",
        "W. J. Zhu",
        "W. Z. Zhu",
        "Y. C. Zhu",
        "Z. A. Zhu",
        "X. Y. Zhuang",
        "J. H. Zou"
      ],
      "abstract": "Using $4.5$ fb$^{-1}$ of $e^+e^-$ collision data collected with the BESIII detector at center-of-mass energies from 4.600 to 4.699 GeV, we report a measurement of the singly Cabibbo-suppressed decay $Λ_c^+ \\to pη'$ with the single-tag method. To effectively distinguish the signal from the large backgrounds, we exploit a deep-learning classifier built on a Transformer-based neural network. Extensive validation and uncertainty quantification are carried out. The $Λ^+_c\\to pη'$ signal is observed with a statistical significance of $3.4 σ$. The ratio of branching fractions of $\\mathcal{B}{Λ^+_c\\to pη'}/\\mathcal{B}{Λ^+_c\\to pω}$= $0.55\\pm 0.22_{\\rm{stat.}} \\pm 0.05_{\\rm{syst.}}$ is obtained, where the first uncertainty is statistical and the second systematic.",
      "published": "February 12, 2026",
      "published_raw": "2026-02-12T14:05:15Z",
      "pdf_link": "http://arxiv.org/pdf/2602.11974v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.11974v1"
    },
    {
      "id": "2602.11648v1",
      "title": "Human-Like Gaze Behavior in Social Robots: A Deep Learning Approach Integrating Human and Non-Human Stimuli",
      "authors": [
        "Faezeh Vahedi",
        "Morteza Memari",
        "Ramtin Tabatabaei",
        "Alireza Taheri"
      ],
      "abstract": "Nonverbal behaviors, particularly gaze direction, play a crucial role in enhancing effective communication in social interactions. As social robots increasingly participate in these interactions, they must adapt their gaze based on human activities and remain receptive to all cues, whether human-generated or not, to ensure seamless and effective communication. This study aims to increase the similarity between robot and human gaze behavior across various social situations, including both human and non-human stimuli (e.g., conversations, pointing, door openings, and object drops). A key innovation in this study, is the investigation of gaze responses to non-human stimuli, a critical yet underexplored area in prior research. These scenarios, were simulated in the Unity software as a 3D animation and a 360-degree real-world video. Data on gaze directions from 41 participants were collected via virtual reality (VR) glasses. Preprocessed data, trained two neural networks-LSTM and Transformer-to build predictive models based on individuals' gaze patterns. In the animated scenario, the LSTM and Transformer models achieved prediction accuracies of 67.6% and 70.4%, respectively; In the real-world scenario, the LSTM and Transformer models achieved accuracies of 72% and 71.6%, respectively. Despite the gaze pattern differences among individuals, our models outperform existing approaches in accuracy while uniquely considering non-human stimuli, offering a significant advantage over previous literature. Furthermore, deployed on the NAO robot, the system was evaluated by 275 participants via a comprehensive questionnaire, with results demonstrating high satisfaction during interactions. This work advances social robotics by enabling robots to dynamically mimic human gaze behavior in complex social contexts.",
      "published": "February 12, 2026",
      "published_raw": "2026-02-12T07:01:17Z",
      "pdf_link": "http://arxiv.org/pdf/2602.11648v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.11648v1"
    },
    {
      "id": "2602.11461v1",
      "title": "EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement & Routing for RF Circuits",
      "authors": [
        "Yilun Huang",
        "Asal Mehradfar",
        "Salman Avestimehr",
        "Hamidreza Aghasi"
      ],
      "abstract": "This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometries with frequency sweeps from 1-100 GHz, generating 7.5 million training samples, that predicts inductor Q-factor with less than 2% error and enables fast gradient-based layout optimization with a 93.77% success rate in producing high-Q layouts; (2) an intelligent P-Cell optimizer that reduces layout area while maintaining design-rule-check (DRC) compliance; and (3) a complete placement and routing engine with frequency-dependent EM spacing rules and DRC-aware synthesis. The neural inductor model demonstrates superior accuracy across 1-100 GHz, enabling EM-accurate component synthesis with real-time inference. The framework successfully generates DRC-aware GDSII layouts for RF circuits, representing a significant step toward automated RF physical design.",
      "published": "February 12, 2026",
      "published_raw": "2026-02-12T00:38:24Z",
      "pdf_link": "http://arxiv.org/pdf/2602.11461v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.11461v1"
    }
  ]
}