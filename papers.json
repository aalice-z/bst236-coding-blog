{
  "last_updated": "2026-02-26T01:00:13.257821",
  "count": 20,
  "papers": [
    {
      "id": "2602.21160v1",
      "title": "Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions",
      "authors": [
        "Mame Diarra Toure",
        "David A. Stephens"
      ],
      "abstract": "In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=σ_k^{2}/(2μ_k)$, with $μ_k{=}\\mathbb{E}[p_k]$ and $σ_k^2{=}\\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/μ_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\\sum_k C_k \\approx \\mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\\% over MI and 56.2\\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T18:05:51Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21160v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21160v1"
    },
    {
      "id": "2602.21046v1",
      "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis",
      "authors": [
        "Kunyu Zhang",
        "Yanwu Yang",
        "Jing Zhang",
        "Xiangjie Shi",
        "Shujian Yu"
      ],
      "abstract": "Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating prototype-based classification and consistency training with structural perturbations during learning. This encourages a structured latent space and enables Monte Carlo Tree Search (MCTS) under a prototype-consistent objective to extract compact minimal-sufficient explanatory subgraphs post-training. Experiments on three benchmark fMRI datasets demonstrate that PIME achieves state-of-the-art performance. Furthermore, by constraining the search space via learned prototypes, PIME identifies critical brain regions that are consistent with established neuroimaging findings. Stability analysis shows 90% reproducibility and consistent explanations across atlases.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T16:04:52Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21046v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21046v1"
    },
    {
      "id": "2602.20947v1",
      "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation",
      "authors": [
        "Thorbjørn Mosekjær Iversen",
        "Zebin Duan",
        "Frederik Hagelskjær"
      ],
      "abstract": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T14:31:28Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20947v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20947v1"
    },
    {
      "id": "2602.20744v1",
      "title": "Voices of the Mountains: Deep Learning-Based Vocal Error Detection System for Kurdish Maqams",
      "authors": [
        "Darvan Shvan Khairaldeen",
        "Hossein Hassani"
      ],
      "abstract": "Maqam, a singing type, is a significant component of Kurdish music. A maqam singer receives training in a traditional face-to-face or through self-training. Automatic Singing Assessment (ASA) uses machine learning (ML) to provide the accuracy of singing styles and can help learners to improve their performance through error detection. Currently, the available ASA tools follow Western music rules. The musical composition requires all notes to stay within their expected pitch range from start to finish. The system fails to detect micro-intervals and pitch bends, so it identifies Kurdish maqam singing as incorrect even though the singer performs according to traditional rules. Kurdish maqam requires recognizing performance errors within microtonal spaces, which is beyond Western equal temperament. This research is the first attempt to address the mentioned gap. While many error types happen during singing, our focus is on pitch, rhythm, and modal stability errors in the context of Bayati-Kurd. We collected 50 songs from 13 vocalists ( 2-3 hours) and annotated 221 error spans (150 fine pitch, 46 rhythm, 25 modal drift). The data was segmented into 15,199 overlapping windows and converted to log-mel spectrograms. We developed a two-headed CNN-BiLSTM with attention mode to decide whether a window contains an error and to classify it based on the chosen errors. Trained for 20 epochs with early stopping at epoch 10, the model reached a validation macro-F1 of 0.468. On the full 50-song evaluation at a 0.750 threshold, recall was 39.4% and precision 25.8% . Within detected windows, type macro-F1 was 0.387, with F1 of 0.492 (fine pitch), 0.536 (rhythm), and 0.133 (modal drift); modal drift recall was 8.0%. The better performance on common error types shows that the method works, while the poor modal-drift recall shows that more data and balancing are needed.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T10:17:16Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20744v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20744v1"
    },
    {
      "id": "2602.20652v1",
      "title": "DANCE: Doubly Adaptive Neighborhood Conformal Estimation",
      "authors": [
        "Brandon R. Feng",
        "Brian J. Reich",
        "Daniel Beaglehole",
        "Xihaier Luo",
        "David Keetae Park",
        "Shinjae Yoo",
        "Zhechao Huang",
        "Xueyu Mao",
        "Olcay Boz",
        "Jungeum Kim"
      ],
      "abstract": "The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T07:54:53Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20652v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20652v1"
    },
    {
      "id": "2602.20651v1",
      "title": "Sparse Bayesian Deep Functional Learning with Structured Region Selection",
      "authors": [
        "Xiaoxian Zhu",
        "Yingmeng Li",
        "Shuangge Ma",
        "Mengyun Wu"
      ],
      "abstract": "In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T07:53:59Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20651v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20651v1"
    },
    {
      "id": "2602.20646v1",
      "title": "On the Convergence of Stochastic Gradient Descent with Perturbed Forward-Backward Passes",
      "authors": [
        "Boao Kong",
        "Hengrui Zhang",
        "Kun Yuan"
      ],
      "abstract": "We study stochastic gradient descent (SGD) for composite optimization problems with $N$ sequential operators subject to perturbations in both the forward and backward passes. Unlike classical analyses that treat gradient noise as additive and localized, perturbations to intermediate outputs and gradients cascade through the computational graph, compounding geometrically with the number of operators. We present the first comprehensive theoretical analysis of this setting. Specifically, we characterize how forward and backward perturbations propagate and amplify within a single gradient step, derive convergence guarantees for both general non-convex objectives and functions satisfying the Polyak--Łojasiewicz condition, and identify conditions under which perturbations do not deteriorate the asymptotic convergence order. As a byproduct, our analysis furnishes a theoretical explanation for the gradient spiking phenomenon widely observed in deep learning, precisely characterizing the conditions under which training recovers from spikes or diverges. Experiments on logistic regression with convex and non-convex regularization validate our theories, illustrating the predicted spike behavior and the asymmetric sensitivity to forward versus backward perturbations.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T07:47:15Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20646v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20646v1"
    },
    {
      "id": "2602.20324v1",
      "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models",
      "authors": [
        "Cathy Shyr",
        "Yan Hu",
        "Rory J. Tinker",
        "Thomas A. Cassini",
        "Kevin W. Byram",
        "Rizwan Hamid",
        "Daniel V. Fabbri",
        "Adam Wright",
        "Josh F. Peterson",
        "Lisa Bastarache",
        "Hua Xu"
      ],
      "abstract": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T20:20:23Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20324v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20324v1"
    },
    {
      "id": "2602.20303v1",
      "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health",
      "authors": [
        "Joyanta Jyoti Mondal"
      ],
      "abstract": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T19:31:44Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20303v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20303v1"
    },
    {
      "id": "2602.20289v1",
      "title": "The Sim-to-Real Gap in MRS Quantification: A Systematic Deep Learning Validation for GABA",
      "authors": [
        "Zien Ma",
        "S. M. Shermer",
        "Oktay Karakuş",
        "Frank C. Langbein"
      ],
      "abstract": "Magnetic resonance spectroscopy (MRS) is used to quantify metabolites in vivo and estimate biomarkers for conditions ranging from neurological disorders to cancers. Quantifying low-concentration metabolites such as GABA ($γ$-aminobutyric acid) is challenging due to low signal-to-noise ratio (SNR) and spectral overlap. We investigate and validate deep learning for quantifying complex, low-SNR, overlapping signals from MEGA-PRESS spectra, devise a convolutional neural network (CNN) and a Y-shaped autoencoder (YAE), and select the best models via Bayesian optimisation on 10,000 simulated spectra from slice-profile-aware MEGA-PRESS simulations. The selected models are trained on 100,000 simulated spectra. We validate their performance on 144 spectra from 112 experimental phantoms containing five metabolites of interest (GABA, Glu, Gln, NAA, Cr) with known ground truth concentrations across solution and gel series acquired at 3 T under varied bandwidths and implementations. These models are further assessed against the widely used LCModel quantification tool. On simulations, both models achieve near-perfect agreement (small MAEs; regression slopes $\\approx 1.00$, $R^2 \\approx 1.00$). On experimental phantom data, errors initially increased substantially. However, modelling variable linewidths in the training data significantly reduced this gap. The best augmented deep learning models achieved a mean MAE for GABA over all phantom spectra of 0.151 (YAE) and 0.160 (FCNN) in max-normalised relative concentrations, outperforming the conventional baseline LCModel (0.220). A sim-to-real gap remains, but physics-informed data augmentation substantially reduced it. Phantom ground truth is needed to judge whether a method will perform reliably on real data.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T19:16:03Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20289v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20289v1"
    },
    {
      "id": "2602.20271v1",
      "title": "Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning",
      "authors": [
        "Stefan Faulkner",
        "Reza Zandehshahvar",
        "Vahid Eghbal Akhlaghi",
        "Sebastien Ouellet",
        "Carsten Jordan",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T19:01:03Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20271v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20271v1"
    },
    {
      "id": "2602.20001v1",
      "title": "FairFS: Addressing Deep Feature Selection Biases for Recommender System",
      "authors": [
        "Xianquan Wang",
        "Zhaocheng Du",
        "Jieming Zhu",
        "Qinglin Jia",
        "Zhenhua Dong",
        "Kai Zhang"
      ],
      "abstract": "Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T16:08:32Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20001v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20001v1"
    },
    {
      "id": "2602.19964v1",
      "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference",
      "authors": [
        "Moritz A. Zanger",
        "Yijun Wu",
        "Pascal R. Van der Vaart",
        "Wendelin Böhmer",
        "Matthijs T. J. Spaan"
      ],
      "abstract": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T15:28:27Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19964v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19964v1"
    },
    {
      "id": "2602.19915v1",
      "title": "Fully Convolutional Spatiotemporal Learning for Microstructure Evolution Prediction",
      "authors": [
        "Michael Trimboli",
        "Mohammed Alsubaie",
        "Sirani M. Perera",
        "Ke-Gang Wang",
        "Xianqi Li"
      ],
      "abstract": "Understanding and predicting microstructure evolution is fundamental to materials science, as it governs the resulting properties and performance of materials. Traditional simulation methods, such as phase-field models, offer high-fidelity results but are computationally expensive due to the need to solve complex partial differential equations at fine spatiotemporal resolutions. To address this challenge, we propose a deep learning-based framework that accelerates microstructure evolution predictions while maintaining high accuracy. Our approach utilizes a fully convolutional spatiotemporal model trained in a self-supervised manner using sequential images generated from simulations of microstructural processes, including grain growth and spinodal decomposition. The trained neural network effectively learns the underlying physical dynamics and can accurately capture both short-term local behaviors and long-term statistical properties of evolving microstructures, while also demonstrating generalization to unseen spatiotemporal domains and variations in configuration and material parameters. Compared to recurrent neural architectures, our model achieves state-of-the-art predictive performance with significantly reduced computational cost in both training and inference. This work establishes a robust baseline for spatiotemporal learning in materials science and offers a scalable, data-driven alternative for fast and reliable microstructure simulations.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T14:55:28Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19915v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19915v1"
    },
    {
      "id": "2602.19775v1",
      "title": "Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization",
      "authors": [
        "Jose M. G. Vilar",
        "Leonor Saiz"
      ],
      "abstract": "Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T12:29:43Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19775v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19775v1"
    },
    {
      "id": "2602.19770v1",
      "title": "The Confusion is Real: GRAPHIC - A Network Science Approach to Confusion Matrices in Deep Learning",
      "authors": [
        "Johanna S. Fröhlich",
        "Bastian Heinlein",
        "Jan U. Claar",
        "Hans Rosenberger",
        "Vasileios Belagiannis",
        "Ralf R. Müller"
      ],
      "abstract": "Explainable artificial intelligence has emerged as a promising field of research to address reliability concerns in artificial intelligence. Despite significant progress in explainable artificial intelligence, few methods provide a systematic way to visualize and understand how classes are confused and how their relationships evolve as training progresses. In this work, we present GRAPHIC, an architecture-agnostic approach that analyzes neural networks on a class level. It leverages confusion matrices derived from intermediate layers using linear classifiers. We interpret these as adjacency matrices of directed graphs, allowing tools from network science to visualize and quantify learning dynamics across training epochs and intermediate layers. GRAPHIC provides insights into linear class separability, dataset issues, and architectural behavior, revealing, for example, similarities between flatfish and man and labeling ambiguities validated in a human study. In summary, by uncovering real confusions, GRAPHIC offers new perspectives on how neural networks learn. The code is available at https://github.com/Johanna-S-Froehlich/GRAPHIC.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T12:20:37Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19770v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19770v1"
    },
    {
      "id": "2602.19691v1",
      "title": "Smoothness Adaptivity in Constant-Depth Neural Networks: Optimal Rates via Smooth Activations",
      "authors": [
        "Yuhao Liu",
        "Zilin Wang",
        "Lei Wu",
        "Shaobo Zhang"
      ],
      "abstract": "Smooth activation functions are ubiquitous in modern deep learning, yet their theoretical advantages over non-smooth counterparts remain poorly understood. In this work, we characterize both approximation and statistical properties of neural networks with smooth activations over the Sobolev space $W^{s,\\infty}([0,1]^d)$ for arbitrary smoothness $s>0$. We prove that constant-depth networks equipped with smooth activations automatically exploit arbitrarily high orders of target function smoothness, achieving the minimax-optimal approximation and estimation error rates (up to logarithmic factors). In sharp contrast, networks with non-smooth activations, such as ReLU, lack this adaptivity: their attainable approximation order is strictly limited by depth, and capturing higher-order smoothness requires proportional depth growth. These results identify activation smoothness as a fundamental mechanism, alternative to depth, for attaining statistical optimality. Technically, our results are established via a constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. This complexity control ensures statistical learnability under empirical risk minimization (ERM) and removes the impractical sparsity constraints commonly required in prior analyses.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T10:38:12Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19691v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19691v1"
    },
    {
      "id": "2602.19584v1",
      "title": "Interpolation-Driven Machine Learning Approaches for Plume Shine Dose Estimation: A Comparison of XGBoost, Random Forest, and TabNet",
      "authors": [
        "Biswajit Sadhu",
        "Kalpak Gupte",
        "Trijit Sadhu",
        "S. Anand"
      ],
      "abstract": "Despite the success of machine learning (ML) in surrogate modeling, its use in radiation dose assessment is limited by safety-critical constraints, scarce training-ready data, and challenges in selecting suitable architectures for physics-dominated systems. Within this context, rapid and accurate plume shine dose estimation serves as a practical test case, as it is critical for nuclear facility safety assessment and radiological emergency response, while conventional photon-transport-based calculations remain computationally expensive. In this work, an interpolation-assisted ML framework was developed using discrete dose datasets generated with the pyDOSEIA suite for 17 gamma-emitting radionuclides across varying downwind distances, release heights, and atmospheric stability categories. The datasets were augmented using shape-preserving interpolation to construct dense, high-resolution training data. Two tree-based ML models (Random Forest and XGBoost) and one deep learning (DL) model (TabNet) were evaluated to examine predictive performance and sensitivity to dataset resolution. All models showed higher prediction accuracy with the interpolated high-resolution dataset than with the discrete data; however, XGBoost consistently achieved the highest accuracy. Interpretability analysis using permutation importance (tree-based models) and attention-based feature attribution (TabNet) revealed that performance differences stem from how the models utilize input features. Tree-based models focus mainly on dominant geometry-dispersion features (release height, stability category, and downwind distance), treating radionuclide identity as a secondary input, whereas TabNet distributes attention more broadly across multiple variables. For practical deployment, a web-based GUI was developed for interactive scenario evaluation and transparent comparison with photon-transport reference calculations.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T08:12:49Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19584v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19584v1"
    },
    {
      "id": "2602.19564v1",
      "title": "Deep Learning Based Monthly Temperature Prediction for Jilin Province: A Multi Model Comparative Study 2000 2026",
      "authors": [
        "Xingyue Deng",
        "Xuechen Liang"
      ],
      "abstract": "Jilin Province, a core commercial grain production base in China with a mid-temperate continental monsoon climate and significant temperature fluctuations, relies heavily on temperature for agricultural production and ecological security. Existing temperature prediction studies focus mostly on national/southeastern coastal regions, with few targeting Jilin's specific climatic characteristics, and most models fail to integrate local temperature's spatiotemporal differentiation and seasonal periodicity, limiting prediction accuracy. Using 1 km $\\times$ 1 km monthly mean temperature raster data (2000--2024) of Jilin Province, we analyzed regional temperature's spatiotemporal variation and constructed a multi-model comparison system including four deep learning models (LSTM, GRU, BiLSTM, Transformer) and five traditional machine learning models (Ridge/Lasso Regression, SVR, Random Forest, Gradient Boosting). Model performance was evaluated via RMSE, MAE, and $R^2$. Results show Jilin's temperature has obvious latitudinal zonal distribution, significant warming trend, strong seasonal periodicity, and high temporal autocorrelation. The LSTM model achieved optimal performance (test set RMSE=2.26 $^\\circ$C, MAE=1.83 $^\\circ$C, $R^2$=0.9655), outperforming traditional models and Transformer. Predictions for 2025--2026 indicate stable seasonal temperature fluctuations with an annual mean of ~4.9 $^\\circ$C. This study enriches mid-latitude cold region temperature prediction research, verifies LSTM's applicability for Jilin's monthly temperature prediction, and provides scientific support for agricultural planning, frost disaster warning, and extreme temperature risk prevention.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T07:25:53Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19564v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19564v1"
    },
    {
      "id": "2602.19531v1",
      "title": "A Statistical Approach for Modeling Irregular Multivariate Time Series with Missing Observations",
      "authors": [
        "Dingyi Nie",
        "Yixing Wu",
        "C. -C. Jay Kuo"
      ],
      "abstract": "Irregular multivariate time series with missing values present significant challenges for predictive modeling in domains such as healthcare. While deep learning approaches often focus on temporal interpolation or complex architectures to handle irregularities, we propose a simpler yet effective alternative: extracting time-agnostic summary statistics to eliminate the temporal axis. Our method computes four key features per variable-mean and standard deviation of observed values, as well as the mean and variability of changes between consecutive observations to create a fixed-dimensional representation. These features are then utilized with standard classifiers, such as logistic regression and XGBoost. Evaluated on four biomedical datasets (PhysioNet Challenge 2012, 2019, PAMAP2, and MIMIC-III), our approach achieves state-of-the-art performance, surpassing recent transformer and graph-based models by 0.5-1.7% in AUROC/AUPRC and 1.1-1.7% in accuracy/F1-score, while reducing computational complexity. Ablation studies demonstrate that feature extraction-not classifier choice-drives performance gains, and our summary statistics outperform raw/imputed input in most benchmarks. In particular, we identify scenarios where missing patterns themselves encode predictive signals, as in sepsis prediction (PhysioNet, 2019), where missing indicators alone can achieve 94.2% AUROC with XGBoost, only 1.6% lower than using original raw data as input. Our results challenge the necessity of complex temporal modeling when task objectives permit time-agnostic representations, providing an efficient and interpretable solution for irregular time series classification.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T05:48:17Z",
      "pdf_link": "http://arxiv.org/pdf/2602.19531v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.19531v1"
    }
  ]
}