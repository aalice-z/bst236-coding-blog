{
  "last_updated": "2026-02-19T01:04:30.021930",
  "count": 20,
  "papers": [
    {
      "id": "2602.15830v1",
      "title": "Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution",
      "authors": [
        "Christopher David Roberts"
      ],
      "abstract": "Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion. We introduce trajectory transformers as a proof-of-concept that ensemble-size independence can be achieved. This approach is an adaptation of the Post-processing Ensembles with Transformers (PoET) framework and applies self-attention over lead time while preserving the conditional independence required by aCRPS. When applied to weekly mean $T_{2m}$ forecasts from the ECMWF subseasonal forecasting system, this approach successfully reduces systematic model biases whilst also improving or maintaining forecast reliability regardless of the ensemble size used in training (3 vs 9 members) or real-time forecasts (9 vs 100 members).",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T18:59:55Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15830v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15830v1"
    },
    {
      "id": "2602.15637v1",
      "title": "The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems",
      "authors": [
        "Amirreza Dolatpour Fathkouhi",
        "Alireza Namazi",
        "Heman Shakeri"
      ],
      "abstract": "Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a systematic \\emph{Stationarity Bias}: simple methods appear superior because the benchmark predominantly samples the easy, low-entropy regime where they trivially succeed. We formalize this bias and propose a \\emph{Stratified Stress-Test} that partitions evaluation into Stationary and Transient regimes. Using Continuous Glucose Monitoring (CGM) as a testbed -- chosen for its rigorous ground-truth forcing functions (meals, insulin) that enable precise regime identification -- we establish three findings with broad implications:(i)~Stationary Efficiency: Linear interpolation achieves state-of-the-art reconstruction during stable intervals, confirming that complex architectures are computationally wasteful in low-entropy regimes.(ii)~Transient Fidelity: During critical transients (post-prandial peaks, hypoglycemic events), linear methods exhibit drastically degraded morphological fidelity (DTW), disproportionate to their RMSE -- a phenomenon we term the \\emph{RMSE Mirage}, where low pointwise error masks the destruction of signal shape.(iii)~Regime-Conditional Model Selection: Deep learning models preserve both pointwise accuracy and morphological integrity during transients, making them essential for safety-critical downstream tasks. We further derive empirical missingness distributions from clinical trials and impose them on complete training data, preventing models from exploiting unrealistically clean observations and encouraging robustness under real-world missingness. This framework generalizes to any regulated system where routine stationarity dominates critical transients.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T15:05:56Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15637v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15637v1"
    },
    {
      "id": "2602.15602v1",
      "title": "Certified Per-Instance Unlearning Using Individual Sensitivity Bounds",
      "authors": [
        "Hanna Benarroch",
        "Jamal Atif",
        "Olivier Cappé"
      ],
      "abstract": "Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate an alternative approach based on adaptive per-instance noise calibration tailored to the individual contribution of each data point to the learned solution. This raises the following challenge: how can one establish formal unlearning guarantees when the mechanism depends on the specific point to be removed? To define individual data point sensitivities in noisy gradient dynamics, we consider the use of per-instance differential privacy. For ridge regression trained via Langevin dynamics, we derive high-probability per-instance sensitivity bounds, yielding certified unlearning with substantially less noise injection. We corroborate our theoretical findings through experiments in linear settings and provide further empirical evidence on the relevance of the approach in deep learning settings.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T14:18:47Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15602v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15602v1"
    },
    {
      "id": "2602.15552v1",
      "title": "Latent Regularization in Generative Test Input Generation",
      "authors": [
        "Giorgi Merabishvili",
        "Oliver Weißl",
        "Andrea Stocco"
      ],
      "abstract": "This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validity, diversity, and fault detection. We evaluate our approach on the boundary testing of deep learning image classifiers across three datasets, MNIST, Fashion MNIST, and CIFAR-10. We compare two truncation strategies: latent code mixing with binary search optimization and random latent truncation for generative exploration. Our experiments show that the latent code-mixing approach yields a higher fault detection rate than random truncation, while also improving both diversity and validity.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T12:57:17Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15552v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15552v1"
    },
    {
      "id": "2602.15484v1",
      "title": "Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction",
      "authors": [
        " Amartyaveer",
        "Murali Kadambi",
        "Chandra Mohan Sharma",
        "Anupam Mondal",
        "Prasanta Kumar Ghosh"
      ],
      "abstract": "In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. To address this, numerous deep learning-based nonintrusive speech assessment models have garnered significant interest. Many studies have achieved commendable performance, but there is room for further improvement. We propose the use of bottleneck transformer, incorporating convolution blocks for learning frame-level features and a multi-head self-attention (MHSA) layer to aggregate the information. These components enable the transformer to focus on the key aspects of the input data. Our model has shown higher correlation and lower mean squared error for both seen and unseen scenarios compared to the state-of-the-art model using self-supervised learning (SSL) and spectral features as inputs.",
      "published": "February 17, 2026",
      "published_raw": "2026-02-17T10:46:54Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15484v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15484v1"
    },
    {
      "id": "2602.15167v1",
      "title": "Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift",
      "authors": [
        "Xiaoyi Wen",
        "Fei Jiang"
      ],
      "abstract": "Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T20:11:42Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15167v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15167v1"
    },
    {
      "id": "2602.15141v1",
      "title": "Mimicking the large-scale structure of the Local Universe. Synthetic pre-labelled galaxies in large-scale structures",
      "authors": [
        "M. Alcázar-Laynez",
        "S. Duarte Puertas",
        "S. Verley",
        "G. Blázquez-Calero",
        "A. Jiménez",
        "A. Lorenzo-Gutiérrez",
        "D. Espada",
        "M. Argudo-Fernández",
        "I. Pérez"
      ],
      "abstract": "Current observational and simulated large-scale structure (LSS) catalogues often lack consistency in assigning galaxies to specific structures, due to the absence of a universally accepted classification criterion. With the aim to generate synthetic empirical data for fine-tuning LSS classification algorithms, as well as to train machine learning (ML)/deep learning (DL) models for the same purpose, this work presents a purely geometrical simulation based on statistical spatial properties found in LSS surveys, using the spectroscopic main galaxy sample of the Sloan Digital Sky Survey (SDSS) catalogue up to a redshift of z~0.1 as a specific use case. A parallelism between the LSS and the Voronoi tessellation was utilised, in which the nodes, links, surfaces, and cells of the diagram correspond to clusters, filaments, walls, and voids, respectively. The simulation used random positions within voids as seeds for tessellating the 3D space. The resulting structures were randomly populated with galaxies that adhere to the statistical properties of their observational respective structures. As the galaxies were generated, they were tagged with their corresponding structure. In each simulation, six LSS mock catalogues were generated, following the statistical behaviour observed in the SDSS catalogue, depending on the structure they belong to. The Malmquist bias and the Fingers of God effect were simulated as well. We present a novel geometrical LSS simulator, where generated galaxies mimic the statistical properties of their observational belonging structure. The simulator was tuned to mimic the SDSS catalogue, although any other catalogue can be considered. With the generated catalogue, it is possible to adjust the LSS classification algorithms, train and test ML/DL models, and benchmark several LSS classification methods using this pre-labelled data to contrast their results and performance.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T19:38:02Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15141v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15141v1"
    },
    {
      "id": "2602.15128v1",
      "title": "PolyNODE: Variable-dimension Neural ODEs on M-polyfolds",
      "authors": [
        "Per Åhag",
        "Alexander Friedrich",
        "Fredrik Ohlsson",
        "Viktor Vigren Näslund"
      ],
      "abstract": "Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally constrained to fixed-dimensional dynamics by the intrinsic nature of the manifold's dimension. In this paper, we extend NODEs to M-polyfolds (spaces that can simultaneously accommodate varying dimensions and a notion of differentiability) and introduce PolyNODEs, the first variable-dimensional flow-based model in geometric deep learning. As an example application, we construct explicit M-polyfolds featuring dimensional bottlenecks and PolyNODE autoencoders based on parametrised vector fields that traverse these bottlenecks. We demonstrate experimentally that our PolyNODE models can be trained to solve reconstruction tasks in these spaces, and that latent representations of the input can be extracted and used to solve downstream classification tasks. The code used in our experiments is publicly available at https://github.com/turbotage/PolyNODE .",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T19:11:06Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15128v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15128v1"
    },
    {
      "id": "2602.14997v1",
      "title": "Spectral Convolution on Orbifolds for Geometric Deep Learning",
      "authors": [
        "Tim Mangliers",
        "Bernhard Mössner",
        "Benjamin Himpel"
      ],
      "abstract": "Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T18:28:38Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14997v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14997v1"
    },
    {
      "id": "2602.14939v1",
      "title": "Fault Detection in Electrical Distribution System using Autoencoders",
      "authors": [
        "Sidharthenee Nayak",
        "Victor Sam Moses Babu",
        "Chandrashekhar Narayan Bhende",
        "Pratyush Chakraborty",
        "Mayukha Pal"
      ],
      "abstract": "In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their practical application remains highly challenging. Given the probabilistic nature of fault occurrences and parameters, certain decision-making tasks could be approached from a probabilistic standpoint. Protective systems are tasked with the detection, classification, and localization of faulty voltage and current line magnitudes, culminating in the activation of circuit breakers to isolate the faulty line. An essential aspect of designing effective fault detection systems lies in obtaining reliable data for training and testing, which is often scarce. Leveraging deep learning techniques, particularly the powerful capabilities of pattern classifiers in learning, generalizing, and parallel processing, offers promising avenues for intelligent fault detection. To address this, our paper proposes an anomaly-based approach for fault detection in electrical power systems, employing deep autoencoders. Additionally, we utilize Convolutional Autoencoders (CAE) for dimensionality reduction, which, due to its fewer parameters, requires less training time compared to conventional autoencoders. The proposed method demonstrates superior performance and accuracy compared to alternative detection approaches by achieving an accuracy of 97.62% and 99.92% on simulated and publicly available datasets.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T17:21:35Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14939v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14939v1"
    },
    {
      "id": "2602.14896v1",
      "title": "Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs",
      "authors": [
        "Pedram Bakhtiarifard",
        "Tong Chen",
        "Jonathan Wenshøj",
        "Erik B Dam",
        "Raghavendra Selvan"
      ],
      "abstract": "Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \\emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\\mathbf{w} \\in \\mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\\mathbf{w}$ by $\\mathcal{K}(\\mathbf{w})$. We introduce a constrained parameterization $\\widehat{\\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T16:30:38Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14896v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14896v1"
    },
    {
      "id": "2602.14853v1",
      "title": "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations",
      "authors": [
        "Jonathan Gorard",
        "Ammar Hakim",
        "James Juno"
      ],
      "abstract": "The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T15:49:19Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14853v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14853v1"
    },
    {
      "id": "2602.14833v1",
      "title": "RF-GPT: Teaching AI to See the Wireless World",
      "authors": [
        "Hang Zou",
        "Yu Tian",
        "Bohao Wang",
        "Lina Bariah",
        "Samson Lasaulce",
        "Chongwen Huang",
        "Mérouane Debbah"
      ],
      "abstract": "Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T15:24:56Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14833v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14833v1"
    },
    {
      "id": "2602.15089v1",
      "title": "Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction",
      "authors": [
        "Takato Yasuno"
      ],
      "abstract": "In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\\% or less and a detection rate of 88--94\\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T15:00:15Z",
      "pdf_link": "http://arxiv.org/pdf/2602.15089v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.15089v1"
    },
    {
      "id": "2602.14759v1",
      "title": "Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training",
      "authors": [
        "Jonathan Lys",
        "Vincent Gripon",
        "Bastien Pasdeloup",
        "Lukas Mauch",
        "Fabien Cardinaux",
        "Ghouthi Boukli Hacene"
      ],
      "abstract": "Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T14:04:24Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14759v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14759v1"
    },
    {
      "id": "2602.14701v1",
      "title": "Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation",
      "authors": [
        "Killian Bakong",
        "Laurent Massoulié",
        "Edouard Oyallon",
        "Kevin Scaman"
      ],
      "abstract": "In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T12:40:59Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14701v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14701v1"
    },
    {
      "id": "2602.14635v1",
      "title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models",
      "authors": [
        "Rohit Raj Rai",
        "Abhishek Dhaka",
        "Amit Awekar"
      ],
      "abstract": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T10:53:02Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14635v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14635v1"
    },
    {
      "id": "2602.14519v1",
      "title": "DeepMTL2R: A Library for Deep Multi-task Learning to Rank",
      "authors": [
        "Chaosheng Dong",
        "Peiyao Xiao",
        "Yijia Wang",
        "Kaiyi Ji"
      ],
      "abstract": "This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \\href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T07:11:38Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14519v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14519v1"
    },
    {
      "id": "2602.14445v1",
      "title": "Selective Synchronization Attention",
      "authors": [
        "Hasi Hays"
      ],
      "abstract": "The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T03:58:12Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14445v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14445v1"
    },
    {
      "id": "2602.14239v1",
      "title": "A Hybrid TGN-SEAL Model for Dynamic Graph Link Prediction",
      "authors": [
        "Nafiseh Sadat Sajadi",
        "Behnam Bahrak",
        "Mahdi Jafari Siavoshani"
      ],
      "abstract": "Predicting links in sparse, continuously evolving networks is a central challenge in network science. Conventional heuristic methods and deep learning models, including Graph Neural Networks (GNNs), are typically designed for static graphs and thus struggle to capture temporal dependencies. Snapshot-based techniques partially address this issue but often encounter data sparsity and class imbalance, particularly in networks with transient interactions such as telecommunication call detail records (CDRs). Temporal Graph Networks (TGNs) model dynamic graphs by updating node embeddings over time; however, their predictive accuracy under sparse conditions remains limited. In this study, we improve the TGN framework by extracting enclosing subgraphs around candidate links, enabling the model to jointly learn structural and temporal information. Experiments on a sparse CDR dataset show that our approach increases average precision by 2.6% over standard TGNs, demonstrating the advantages of integrating local topology for robust link prediction in dynamic networks.",
      "published": "February 15, 2026",
      "published_raw": "2026-02-15T17:16:47Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14239v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14239v1"
    }
  ]
}