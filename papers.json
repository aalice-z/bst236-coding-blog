{
  "last_updated": "2026-03-01T01:08:38.267908",
  "count": 20,
  "papers": [
    {
      "id": "2602.23182v1",
      "title": "Closing the gap on tabular data with Fourier and Implicit Categorical Features",
      "authors": [
        "Marius Dragoi",
        "Florin Gogianu",
        "Elena Burceanu"
      ],
      "abstract": "While Deep Learning has demonstrated impressive results in applications on various data types, it continues to lag behind tree-based methods when applied to tabular data, often referred to as the last \"unconquered castle\" for neural networks. We hypothesize that a significant advantage of tree-based methods lies in their intrinsic capability to model and exploit non-linear interactions induced by features with categorical characteristics. In contrast, neural-based methods exhibit biases toward uniform numerical processing of features and smooth solutions, making it challenging for them to effectively leverage such patterns. We address this performance gap by using statistical-based feature processing techniques to identify features that are strongly correlated with the target once discretized. We further mitigate the bias of deep models for overly-smooth solutions, a bias that does not align with the inherent properties of the data, using Learned Fourier. We show that our proposed feature preprocessing significantly boosts the performance of deep learning models and enables them to achieve a performance that closely matches or surpasses XGBoost on a comprehensive tabular data benchmark.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T16:40:23Z",
      "pdf_link": "http://arxiv.org/pdf/2602.23182v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.23182v1"
    },
    {
      "id": "2602.23128v1",
      "title": "Bound to Disagree: Generalization Bounds via Certifiable Surrogates",
      "authors": [
        "Mathieu Bazinet",
        "Valentina Zantedeschi",
        "Pascal Germain"
      ],
      "abstract": "Generalization bounds for deep learning models are typically vacuous, not computable or restricted to specific model classes. In this paper, we tackle these issues by providing new disagreement-based certificates for the gap between the true risk of any two predictors. We then bound the true risk of the predictor of interest via a surrogate model that enjoys tight generalization guarantees, and evaluating our disagreement bound on an unlabeled dataset. We empirically demonstrate the tightness of the obtained certificates and showcase the versatility of the approach by training surrogate models leveraging three different frameworks: sample compression, model compression and PAC-Bayes theory. Importantly, such guarantees are achieved without modifying the target model, nor adapting the training procedure to the generalization framework.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T15:42:13Z",
      "pdf_link": "http://arxiv.org/pdf/2602.23128v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.23128v1"
    },
    {
      "id": "2602.23089v1",
      "title": "Physics-informed neural particle flow for the Bayesian update step",
      "authors": [
        "Domonkos Csuzdi",
        "Tamás Bécsi",
        "Olivér Törő"
      ],
      "abstract": "The Bayesian update step poses significant computational challenges in high-dimensional nonlinear estimation. While log-homotopy particle flow filters offer an alternative to stochastic sampling, existing formulations usually yield stiff differential equations. Conversely, existing deep learning approximations typically treat the update as a black-box task or rely on asymptotic relaxation, neglecting the exact geometric structure of the finite-horizon probability transport. In this work, we propose a physics-informed neural particle flow, which is an amortized inference framework. To construct the flow, we couple the log-homotopy trajectory of the prior to posterior density function with the continuity equation describing the density evolution. This derivation yields a governing partial differential equation (PDE), referred to as the master PDE. By embedding this PDE as a physical constraint into the loss function, we train a neural network to approximate the transport velocity field. This approach enables purely unsupervised training, eliminating the need for ground-truth posterior samples. We demonstrate that the neural parameterization acts as an implicit regularizer, mitigating the numerical stiffness inherent to analytic flows and reducing online computational complexity. Experimental validation on multimodal benchmarks and a challenging nonlinear scenario confirms better mode coverage and robustness compared to state-of-the-art baselines.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T15:10:45Z",
      "pdf_link": "http://arxiv.org/pdf/2602.23089v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.23089v1"
    },
    {
      "id": "2602.22895v1",
      "title": "SPD Learn: A Geometric Deep Learning Python Library for Neural Decoding Through Trivialization",
      "authors": [
        "Bruno Aristimunha",
        "Ce Ju",
        "Antoine Collas",
        "Florent Bouchard",
        "Ammar Mian",
        "Bertrand Thirion",
        "Sylvain Chevallier",
        "Reinmar Kobler"
      ],
      "abstract": "Implementations of symmetric positive definite (SPD) matrix-based neural networks for neural decoding remain fragmented across research codebases and Python packages. Existing implementations often employ ad hoc handling of manifold constraints and non-unified training setups, which hinders reproducibility and integration into modern deep-learning workflows. To address this gap, we introduce SPD Learn, a unified and modular Python package for geometric deep learning with SPD matrices. SPD Learn provides core SPD operators and neural-network layers, including numerically stable spectral operators, and enforces Stiefel/SPD constraints via trivialization-based parameterizations. This design enables standard backpropagation and optimization in unconstrained Euclidean spaces while producing manifold-constrained parameters by construction. The package also offers reference implementations of representative SPDNet-based models and interfaces with widely used brain computer interface/neuroimaging toolkits and modern machine-learning libraries (e.g., MOABB, Braindecode, Nilearn, and SKADA), facilitating reproducible benchmarking and practical deployment.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T11:33:21Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22895v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22895v1"
    },
    {
      "id": "2602.22850v1",
      "title": "MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction",
      "authors": [
        "Yi He",
        "Yina Cao",
        "Jixiu Zhai",
        "Di Wang",
        "Junxiao Kong",
        "Tianchi Lu"
      ],
      "abstract": "Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its \"black-box\" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a \"sequence-structure synergy\" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T10:38:41Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22850v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22850v1"
    },
    {
      "id": "2602.22822v1",
      "title": "FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics",
      "authors": [
        "Yunhua Zhong",
        "Yixuan Tang",
        "Yifan Li",
        "Jie Yang",
        "Pan Liu",
        "Jun Xia"
      ],
      "abstract": "The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T10:05:01Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22822v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22822v1"
    },
    {
      "id": "2602.22777v1",
      "title": "KMLP: A Scalable Hybrid Architecture for Web-Scale Tabular Data Modeling",
      "authors": [
        "Mingming Zhang",
        "Pengfei Shi",
        "Zhiqing Xiao",
        "Feng Zhao",
        "Guandong Sun",
        "Yulin Kang",
        "Ruizhe Gao",
        "Ningtao Wang",
        "Xing Fu",
        "Weiqiang Wang",
        "Junbo Zhao"
      ],
      "abstract": "Predictive modeling on web-scale tabular data with billions of instances and hundreds of heterogeneous numerical features faces significant scalability challenges. These features exhibit anisotropy, heavy-tailed distributions, and non-stationarity, creating bottlenecks for models like Gradient Boosting Decision Trees and requiring laborious manual feature engineering. We introduce KMLP, a hybrid deep architecture integrating a shallow Kolmogorov-Arnold Network (KAN) front-end with a Gated Multilayer Perceptron (gMLP) backbone. The KAN front-end uses learnable activation functions to automatically model complex non-linear transformations for each feature, while the gMLP backbone captures high-order interactions. Experiments on public benchmarks and an industrial dataset with billions of samples show KMLP achieves state-of-the-art performance, with advantages over baselines like GBDTs increasing at larger scales, validating KMLP as a scalable deep learning paradigm for large-scale web tabular data.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T09:12:12Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22777v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22777v1"
    },
    {
      "id": "2602.22552v1",
      "title": "Relatron: Automating Relational Machine Learning over Relational Databases",
      "authors": [
        "Zhikai Chen",
        "Han Xie",
        "Jian Zhang",
        "Jiliang Tang",
        "Xiang Song",
        "Huzefa Rangwala"
      ],
      "abstract": "Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the \"more tuning, worse performance\" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T02:45:22Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22552v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22552v1"
    },
    {
      "id": "2602.22544v1",
      "title": "HARU-Net: Hybrid Attention Residual U-Net for Edge-Preserving Denoising in Cone-Beam Computed Tomography",
      "authors": [
        "Khuram Naveed",
        "Ruben Pauwels"
      ],
      "abstract": "Cone-beam computed tomography (CBCT) is widely used in dental and maxillofacial imaging, but low-dose acquisition introduces strong, spatially varying noise that degrades soft-tissue visibility and obscures fine anatomical structures. Classical denoising methods struggle to suppress noise in CBCT while preserving edges. Although deep learning-based approaches offer high-fidelity restoration, their use in CBCT denoising is limited by the scarcity of high-resolution CBCT data for supervised training. To address this research gap, we propose a novel Hybrid Attention Residual U-Net (HARU-Net) for high-quality denoising of CBCT data, trained on a cadaver dataset of human hemimandibles acquired using a high-resolution protocol of the 3D Accuitomo 170 (J. Morita, Kyoto, Japan) CBCT system. The novel contribution of this approach is the integration of three complementary architectural components: (i) a hybrid attention transformer block (HAB) embedded within each skip connection to selectively emphasize salient anatomical features, (ii) a residual hybrid attention transformer group (RHAG) at the bottleneck to strengthen global contextual modeling and long-range feature interactions, and (iii) residual learning convolutional blocks to facilitate deeper, more stable feature extraction throughout the network. HARU-Net consistently outperforms state-of-the-art (SOTA) methods including SwinIR and Uformer, achieving the highest PSNR (37.52 dB), highest SSIM (0.9557), and lowest GMSD (0.1084). This effective and clinically reliable CBCT denoising is achieved at a computational cost significantly lower than that of the SOTA methods, offering a practical advancement toward improving diagnostic quality in low-dose CBCT imaging.",
      "published": "February 26, 2026",
      "published_raw": "2026-02-26T02:36:34Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22544v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22544v1"
    },
    {
      "id": "2602.22387v1",
      "title": "Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization",
      "authors": [
        "Yixuan Li",
        "Archer Y. Yang",
        "Yue Li"
      ],
      "abstract": "Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \\model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \\model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T20:34:07Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22387v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22387v1"
    },
    {
      "id": "2602.22345v1",
      "title": "Structure and Redundancy in Large Language Models: A Spectral Study via Random Matrix Theory",
      "authors": [
        "Davide Ettori"
      ],
      "abstract": "This thesis addresses two persistent and closely related challenges in modern deep learning, reliability and efficiency, through a unified framework grounded in Spectral Geometry and Random Matrix Theory (RMT). As deep networks and large language models continue to scale, their internal behavior becomes increasingly opaque, leading to hallucinations, fragile generalization under distribution shift, and growing computational and energy demands. By analyzing the eigenvalue dynamics of hidden activations across layers and inputs, this work shows that spectral statistics provide a compact, stable, and interpretable lens on model behavior, capable of separating structured, causal representations from noise-dominated variability. Within this framework, the first contribution, EigenTrack, introduces a real-time method for detecting hallucinations and out-of-distribution behavior in large language and vision-language models. EigenTrack transforms streaming activations into spectral descriptors such as entropy, variance, and deviations from the Marchenko-Pastur baseline, and models their temporal evolution using lightweight recurrent classifiers, enabling early detection of reliability failures before they appear in model outputs while offering interpretable insight into representation dynamics. The second contribution, RMT-KD, presents a principled approach to compressing deep networks via random matrix theoretic knowledge distillation. By interpreting outlier eigenvalues in activation spectra as carriers of task-relevant information, RMT-KD progressively projects networks onto lower-dimensional subspaces through iterative self-distillation, yielding significantly more compact and energy-efficient models while preserving accuracy and dense, hardware-friendly structure.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T19:11:56Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22345v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22345v1"
    },
    {
      "id": "2602.22015v1",
      "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
      "authors": [
        "Pengcheng Hao",
        "Ercan Engin Kuruoglu"
      ],
      "abstract": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T15:29:44Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22015v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22015v1"
    },
    {
      "id": "2602.21961v1",
      "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
      "authors": [
        "Bendegúz Sulyok",
        "Gergely Palla",
        "Filippo Radicchi",
        "Santo Fortunato"
      ],
      "abstract": "We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of the robustness of these networks, exploring their performance under various perturbations including random link removal, adversarial attack, and link weight shuffling. Through extensive experiments, we demonstrate that adaptive topology not only enhances efficiency but also maintains robustness. This work highlights the potential of adaptive sparse networks as a promising direction for developing efficient and reliable deep learning models.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T14:44:15Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21961v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21961v1"
    },
    {
      "id": "2602.21876v1",
      "title": "Comparative Evaluation of Machine Learning Models for Predicting Donor Kidney Discard",
      "authors": [
        "Peer Schliephacke",
        "Hannah Schult",
        "Leon Mizera",
        "Judith Würfel",
        "Gunter Grieser",
        "Axel Rahmel",
        "Carl-Ludwig Fischer-Fröhlich",
        "Antje Jahn-Eimermacher"
      ],
      "abstract": "A kidney transplant can improve the life expectancy and quality of life of patients with end-stage renal failure. Even more patients could be helped with a transplant if the rate of kidneys that are discarded and not transplanted could be reduced. Machine learning (ML) can support decision-making in this context by early identification of donor organs at high risk of discard, for instance to enable timely interventions to improve organ utilization such as rescue allocation. Although various ML models have been applied, their results are difficult to compare due to heterogenous datasets and differences in feature engineering and evaluation strategies. This study aims to provide a systematic and reproducible comparison of ML models for donor kidney discard prediction. We trained five commonly used ML models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Deep Learning along with an ensemble model on data from 4,080 deceased donors (death determined by neurologic criteria) in Germany. A unified benchmarking framework was implemented, including standardized feature engineering and selection, and Bayesian hyperparameter optimization. Model performance was assessed for discrimination (MCC, AUC, F1), calibration (Brier score), and explainability (SHAP). The ensemble achieved the highest discrimination performance (MCC=0.76, AUC=0.87, F1=0.90), while individual models such as Logistic Regression, Random Forest, and Deep Learning performed comparably and better than Decision Trees. Platt scaling improved calibration for tree-and neural network-based models. SHAP consistently identified donor age and renal markers as dominant predictors across models, reflecting clinical plausibility. This study demonstrates that consistent data preprocessing, feature selection, and evaluation can be more decisive for predictive success than the choice of the ML algorithm.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T13:00:05Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21876v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21876v1"
    },
    {
      "id": "2602.22284v1",
      "title": "BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning",
      "authors": [
        "Mingi Kim",
        "Yongjun Kim",
        "Jungwoo Kang",
        "Hyungki Kim"
      ],
      "abstract": "Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T12:44:28Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22284v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22284v1"
    },
    {
      "id": "2602.21761v1",
      "title": "Survey on Neural Routing Solvers",
      "authors": [
        "Yunpeng Ba",
        "Xi Lin",
        "Changliang Zhou",
        "Ruihao Zheng",
        "Zhenkun Wang",
        "Xinyan Liang",
        "Zhichao Lu",
        "Jianyong Sun",
        "Yuhua Qian",
        "Qingfu Zhang"
      ],
      "abstract": "Neural routing solvers (NRSs) that leverage deep learning to tackle vehicle routing problems have demonstrated notable potential for practical applications. By learning implicit heuristic rules from data, NRSs replace the handcrafted counterparts in classic heuristic frameworks, thereby reducing reliance on costly manual design and trial-and-error adjustments. This survey makes two main contributions: (1) The heuristic nature of NRSs is highlighted, and existing NRSs are reviewed from the perspective of heuristics. A hierarchical taxonomy based on heuristic principles is further introduced. (2) A generalization-focused evaluation pipeline is proposed to address limitations of the conventional pipeline. Comparative benchmarking of representative NRSs across both pipelines uncovers a series of previously unreported gaps in current research.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T10:24:43Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21761v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21761v1"
    },
    {
      "id": "2602.22277v1",
      "title": "X-REFINE: XAI-based RElevance input-Filtering and archItecture fiNe-tuning for channel Estimation",
      "authors": [
        "Abdul Karim Gizzini",
        "Yahia Medjahdi"
      ],
      "abstract": "AI-native architectures are vital for 6G wireless communications. The black-box nature and high complexity of deep learning models employed in critical applications, such as channel estimation, limit their practical deployment. While perturbation-based XAI solutions offer input filtering, they often neglect internal structural optimization. We propose X-REFINE, an XAI-based framework for joint input-filtering and architecture fine-tuning. By utilizing a decomposition-based, sign-stabilized LRP epsilon rule, X-REFINE backpropagates predictions to derive high-resolution relevance scores for both subcarriers and hidden neurons. This enables a holistic optimization that identifies the most faithful model components. Simulation results demonstrate that X-REFINE achieves a superior interpretability-performance-complexity trade-off, significantly reducing computational complexity while maintaining robust bit error rate (BER) performance across different scenarios.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T10:20:26Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22277v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22277v1"
    },
    {
      "id": "2602.21757v1",
      "title": "Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction",
      "authors": [
        "Xiannan Huang",
        "Quan Yuan",
        "Chao Yang"
      ],
      "abstract": "Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T10:19:39Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21757v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21757v1"
    },
    {
      "id": "2602.22275v1",
      "title": "Deep Accurate Solver for the Geodesic Problem",
      "authors": [
        "Saar Huberman",
        "Amit Bracha",
        "Ron Kimmel"
      ],
      "abstract": "A common approach to compute distances on continuous surfaces is by considering a discretized polygonal mesh approximating the surface and estimating distances on the polygon. We show that exact geodesic distances restricted to the polygon are at most second-order accurate with respect to the distances on the corresponding continuous surface. By order of accuracy we refer to the convergence rate as a function of the average distance between sampled points. Next, a higher-order accurate deep learning method for computing geodesic distances on surfaces is introduced. Traditionally, one considers two main components when computing distances on surfaces: a numerical solver that locally approximates the distance function, and an efficient causal ordering scheme by which surface points are updated. Classical minimal path methods often exploit a dynamic programming principle with quasi-linear computational complexity in the number of sampled points. The quality of the distance approximation is determined by the local solver that is revisited in this paper. To improve state of the art accuracy, we consider a neural network-based local solver which implicitly approximates the structure of the continuous surface. We supply numerical evidence that the proposed learned update scheme provides better accuracy compared to the best possible polyhedral approximations and previous learning-based methods. The result is a third-order accurate solver with a bootstrapping-recipe for further improvement.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T09:39:49Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22275v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22275v1"
    },
    {
      "id": "2602.21707v1",
      "title": "Learning spatially adaptive sparsity level maps for arbitrary convolutional dictionaries",
      "authors": [
        "Joshua Schulz",
        "David Schote",
        "Christoph Kolbitsch",
        "Kostas Papafitsoros",
        "Andreas Kofler"
      ],
      "abstract": "State-of-the-art learned reconstruction methods often rely on black-box modules that, despite their strong performance, raise questions about their interpretability and robustness. Here, we build on a recently proposed image reconstruction method, which is based on embedding data-driven information into a model-based convolutional dictionary regularization via neural network-inferred spatially adaptive sparsity level maps. By means of improved network design and dedicated training strategies, we extend the method to achieve filter-permutation invariance as well as the possibility to change the convolutional dictionary at inference time. We apply our method to low-field MRI and compare it to several other recent deep learning-based methods, also on in vivo data, in which the benefit for the use of a different dictionary is showcased. We further assess the method's robustness when tested on in- and out-of-distribution data. When tested on the latter, the proposed method suffers less from the data distribution shift compared to the other learned methods, which we attribute to its reduced reliance on training data due to its underlying model-based reconstruction component.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T09:13:24Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21707v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21707v1"
    }
  ]
}