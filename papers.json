{
  "last_updated": "2026-02-17T21:43:15.684342",
  "count": 20,
  "papers": [
    {
      "id": "2602.14997v1",
      "title": "Spectral Convolution on Orbifolds for Geometric Deep Learning",
      "authors": [
        "Tim Mangliers",
        "Bernhard Mössner",
        "Benjamin Himpel"
      ],
      "abstract": "Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T18:28:38Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14997v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14997v1"
    },
    {
      "id": "2602.14939v1",
      "title": "Fault Detection in Electrical Distribution System using Autoencoders",
      "authors": [
        "Sidharthenee Nayak",
        "Victor Sam Moses Babu",
        "Chandrashekhar Narayan Bhende",
        "Pratyush Chakraborty",
        "Mayukha Pal"
      ],
      "abstract": "In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their practical application remains highly challenging. Given the probabilistic nature of fault occurrences and parameters, certain decision-making tasks could be approached from a probabilistic standpoint. Protective systems are tasked with the detection, classification, and localization of faulty voltage and current line magnitudes, culminating in the activation of circuit breakers to isolate the faulty line. An essential aspect of designing effective fault detection systems lies in obtaining reliable data for training and testing, which is often scarce. Leveraging deep learning techniques, particularly the powerful capabilities of pattern classifiers in learning, generalizing, and parallel processing, offers promising avenues for intelligent fault detection. To address this, our paper proposes an anomaly-based approach for fault detection in electrical power systems, employing deep autoencoders. Additionally, we utilize Convolutional Autoencoders (CAE) for dimensionality reduction, which, due to its fewer parameters, requires less training time compared to conventional autoencoders. The proposed method demonstrates superior performance and accuracy compared to alternative detection approaches by achieving an accuracy of 97.62% and 99.92% on simulated and publicly available datasets.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T17:21:35Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14939v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14939v1"
    },
    {
      "id": "2602.14896v1",
      "title": "Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs",
      "authors": [
        "Pedram Bakhtiarifard",
        "Tong Chen",
        "Jonathan Wenshøj",
        "Erik B Dam",
        "Raghavendra Selvan"
      ],
      "abstract": "Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \\emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\\mathbf{w} \\in \\mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\\mathbf{w}$ by $\\mathcal{K}(\\mathbf{w})$. We introduce a constrained parameterization $\\widehat{\\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T16:30:38Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14896v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14896v1"
    },
    {
      "id": "2602.14853v1",
      "title": "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations",
      "authors": [
        "Jonathan Gorard",
        "Ammar Hakim",
        "James Juno"
      ],
      "abstract": "The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T15:49:19Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14853v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14853v1"
    },
    {
      "id": "2602.14833v1",
      "title": "RF-GPT: Teaching AI to See the Wireless World",
      "authors": [
        "Hang Zou",
        "Yu Tian",
        "Bohao Wang",
        "Lina Bariah",
        "Samson Lasaulce",
        "Chongwen Huang",
        "Mérouane Debbah"
      ],
      "abstract": "Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T15:24:56Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14833v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14833v1"
    },
    {
      "id": "2602.14759v1",
      "title": "Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training",
      "authors": [
        "Jonathan Lys",
        "Vincent Gripon",
        "Bastien Pasdeloup",
        "Lukas Mauch",
        "Fabien Cardinaux",
        "Ghouthi Boukli Hacene"
      ],
      "abstract": "Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T14:04:24Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14759v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14759v1"
    },
    {
      "id": "2602.14701v1",
      "title": "Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation",
      "authors": [
        "Killian Bakong",
        "Laurent Massoulié",
        "Edouard Oyallon",
        "Kevin Scaman"
      ],
      "abstract": "In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T12:40:59Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14701v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14701v1"
    },
    {
      "id": "2602.14635v1",
      "title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models",
      "authors": [
        "Rohit Raj Rai",
        "Abhishek Dhaka",
        "Amit Awekar"
      ],
      "abstract": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T10:53:02Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14635v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14635v1"
    },
    {
      "id": "2602.14519v1",
      "title": "DeepMTL2R: A Library for Deep Multi-task Learning to Rank",
      "authors": [
        "Chaosheng Dong",
        "Peiyao Xiao",
        "Yijia Wang",
        "Kaiyi Ji"
      ],
      "abstract": "This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \\href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T07:11:38Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14519v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14519v1"
    },
    {
      "id": "2602.14445v1",
      "title": "Selective Synchronization Attention",
      "authors": [
        "Hasi Hays"
      ],
      "abstract": "The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.",
      "published": "February 16, 2026",
      "published_raw": "2026-02-16T03:58:12Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14445v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14445v1"
    },
    {
      "id": "2602.14239v1",
      "title": "A Hybrid TGN-SEAL Model for Dynamic Graph Link Prediction",
      "authors": [
        "Nafiseh Sadat Sajadi",
        "Behnam Bahrak",
        "Mahdi Jafari Siavoshani"
      ],
      "abstract": "Predicting links in sparse, continuously evolving networks is a central challenge in network science. Conventional heuristic methods and deep learning models, including Graph Neural Networks (GNNs), are typically designed for static graphs and thus struggle to capture temporal dependencies. Snapshot-based techniques partially address this issue but often encounter data sparsity and class imbalance, particularly in networks with transient interactions such as telecommunication call detail records (CDRs). Temporal Graph Networks (TGNs) model dynamic graphs by updating node embeddings over time; however, their predictive accuracy under sparse conditions remains limited. In this study, we improve the TGN framework by extracting enclosing subgraphs around candidate links, enabling the model to jointly learn structural and temporal information. Experiments on a sparse CDR dataset show that our approach increases average precision by 2.6% over standard TGNs, demonstrating the advantages of integrating local topology for robust link prediction in dynamic networks.",
      "published": "February 15, 2026",
      "published_raw": "2026-02-15T17:16:47Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14239v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14239v1"
    },
    {
      "id": "2602.14208v1",
      "title": "Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws",
      "authors": [
        "Jinbo Wang",
        "Binghui Li",
        "Zhanpeng Zhou",
        "Mingze Wang",
        "Yuxuan Sun",
        "Jiaqi Zhang",
        "Xunliang Cai",
        "Lei Wu"
      ],
      "abstract": "Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.",
      "published": "February 15, 2026",
      "published_raw": "2026-02-15T16:06:45Z",
      "pdf_link": "http://arxiv.org/pdf/2602.14208v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.14208v1"
    },
    {
      "id": "2602.13930v1",
      "title": "MamaDino: A Hybrid Vision Model for Breast Cancer 3-Year Risk Prediction",
      "authors": [
        "Ruggiero Santeramo",
        "Igor Zubarev",
        "Florian Jug"
      ],
      "abstract": "Breast cancer screening programmes increasingly seek to move from one-size-fits-all interval to risk-adapted and personalized strategies. Deep learning (DL) has enabled image-based risk models with stronger 1- to 5-year prediction than traditional clinical models, but leading systems (e.g., Mirai) typically use convolutional backbones, very high-resolution inputs (>1M pixels) and simple multi-view fusion, with limited explicit modelling of contralateral asymmetry. We hypothesised that combining complementary inductive biases (convolutional and transformer-based) with explicit contralateral asymmetry modelling would allow us to match state-of-the-art 3-year risk prediction performance even when operating on substantially lower-resolution mammograms, indicating that using less detailed images in a more structured way can recover state-of-the-art accuracy. We present MamaDino, a mammography-aware multi-view attentional DINO model. MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output a 3-year breast cancer risk score. We train on 53,883 women from OPTIMAM (UK) and evaluate on matched 3-year case-control cohorts: an in-distribution test set from four screening sites and an external out-of-distribution cohort from an unseen site. At breast-level, MamaDino matches Mirai on both internal and external tests while using ~13x fewer input pixels. Adding the BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across age, ethnicity, scanner, tumour type and grade. These findings demonstrate that explicit contralateral modelling and complementary inductive biases enable predictions that match Mirai, despite operating on substantially lower-resolution mammograms.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T23:56:22Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13930v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13930v1"
    },
    {
      "id": "2602.13690v1",
      "title": "Physics Aware Neural Networks: Denoising for Magnetic Navigation",
      "authors": [
        "Aritra Das",
        "Yashas Shende",
        "Muskaan Chugh",
        "Reva Laxmi Chauhan",
        "Arghya Pathak",
        "Debayan Gupta"
      ],
      "abstract": "Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T09:23:57Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13690v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13690v1"
    },
    {
      "id": "2602.13634v1",
      "title": "Optimization-Free Graph Embedding via Distributional Kernel for Community Detection",
      "authors": [
        "Shuaibin Song",
        "Kai Ming Ting",
        "Kaifeng Zhang",
        "Tianrun Liang"
      ],
      "abstract": "Neighborhood Aggregation Strategy (NAS) is a widely used approach in graph embedding, underpinning both Graph Neural Networks (GNNs) and Weisfeiler-Lehman (WL) methods. However, NAS-based methods are identified to be prone to over-smoothing-the loss of node distinguishability with increased iterations-thereby limiting their effectiveness. This paper identifies two characteristics in a network, i.e., the distributions of nodes and node degrees that are critical for expressive representation but have been overlooked in existing methods. We show that these overlooked characteristics contribute significantly to over-smoothing of NAS-methods. To address this, we propose a novel weighted distribution-aware kernel that embeds nodes while taking their distributional characteristics into consideration. Our method has three distinguishing features: (1) it is the first method to explicitly incorporate both distributional characteristics; (2) it requires no optimization; and (3) it effectively mitigates the adverse effects of over-smoothing, allowing WL to preserve node distinguishability and expressiveness even after many iterations of embedding. Experiments demonstrate that our method achieves superior community detection performance via spectral clustering, outperforming existing graph embedding methods, including deep learning methods, on standard benchmarks.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T06:56:40Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13634v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13634v1"
    },
    {
      "id": "2602.13601v1",
      "title": "Resolving Cryogenic and Hypersonic Rarefied Flows via Deep Learning-Accelerated Lennard-Jones DSMC",
      "authors": [
        "Ahmad Shoja Sani",
        "Ehsan Roohi",
        "Stefan Stefanov"
      ],
      "abstract": "Integrating a physically realistic Lennard Jones LJ potential into Direct Simulation Monte Carlo DSMC has long been hindered by the high cost of evaluating detailed scattering dynamics. We present a high-fidelity, machine-learning-accelerated framework that bridges rigorous molecular physics and large-scale kinetic simulation, implemented within Bird standard DSMC algorithm suite. Two challenges are solved incorporating LJ consistent properties into DSMC total cross-section formulation, and replacing the expensive particle scattering step with a surrogate model. First, we develop a universal Variable Effective Diameter model via local viscosity matching, capturing attractive repulsive interactions over a wide temperature range an advance over traditional models restricted to narrow thermal bands. Second, we employ a Deep Operator Network as a fast, accurate substitute for the LJ scattering integral, enabling efficient high-precision collisions. The resulting framework exposes physical effects often missed by standard models and is validated on three canonical problems: shock waves in helium and argon, supersonic Couette flow with cryogenic walls, and hypersonic cylinder flow at two Mach numbers. In the argon shock case, we show that while the Variable Hard Sphere VHS model fails to match the experimental density profile, its velocity distribution function closely follows the LJ prediction. In low temperature supersonic Couette flow, the LJ model predicts smaller shear stress than VHS, underscoring the dominant influence of long-range attractive forces in cryogenic shear layers.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T04:47:28Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13601v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13601v1"
    },
    {
      "id": "2602.13550v1",
      "title": "Out-of-Support Generalisation via Weight Space Sequence Modelling",
      "authors": [
        "Roussel Desmond Nzoyem"
      ],
      "abstract": "As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.",
      "published": "February 14, 2026",
      "published_raw": "2026-02-14T01:51:54Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13550v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13550v1"
    },
    {
      "id": "2602.13455v1",
      "title": "Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety",
      "authors": [
        "Phyllis Nabangi",
        "Abdul-Jalil Zakaria",
        "Jema David Ndibwile"
      ],
      "abstract": "The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated language in Swahili, a low-resource language that poses unique challenges due to its limited linguistic resources and technological support. Swahili is chosen due to its popularity and being the most widely spoken language in Africa, with over 16 million native speakers and upwards of 100 million speakers in total, spanning regions in East Africa and some parts of the Middle East. We employed machine learning models including Support Vector Machines (SVM), Logistic Regression, and Decision Trees, optimized through rigorous parameter tuning and techniques like Synthetic Minority Over-sampling Technique (SMOTE) to handle data imbalance. Our analysis revealed that, while these models perform well in high-dimensional textual data, our dataset's small size and imbalance limit our findings' generalizability. Precision, recall, and F1 scores were thoroughly analyzed, highlighting the nuanced performance of each model in detecting obfuscated language. This research contributes to the broader discourse on ensuring safer online environments for children, advocating for expanded datasets and advanced machine-learning techniques to improve the effectiveness of cyberbullying detection systems. Future work will focus on enhancing data robustness, exploring transfer learning, and integrating multimodal data to create more comprehensive and culturally sensitive detection mechanisms.",
      "published": "February 13, 2026",
      "published_raw": "2026-02-13T21:02:14Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13455v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13455v1"
    },
    {
      "id": "2602.13416v1",
      "title": "High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator",
      "authors": [
        "Haiwen Guan",
        "Moein Darman",
        "Dibyajyoti Chakraborty",
        "Troy Arcomano",
        "Ashesh Chattopadhyay",
        "Romit Maulik"
      ],
      "abstract": "The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.",
      "published": "February 13, 2026",
      "published_raw": "2026-02-13T19:36:46Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13416v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13416v1"
    },
    {
      "id": "2602.13087v1",
      "title": "EXCODER: EXplainable Classification Of DiscretE time series Representations",
      "authors": [
        "Yannik Hahn",
        "Antonin Königsfeld",
        "Hasan Tercan",
        "Tobias Meisen"
      ],
      "abstract": "Deep learning has significantly improved time series classification, yet the lack of explainability in these models remains a major challenge. While Explainable AI (XAI) techniques aim to make model decisions more transparent, their effectiveness is often hindered by the high dimensionality and noise present in raw time series data. In this work, we investigate whether transforming time series into discrete latent representations-using methods such as Vector Quantized Variational Autoencoders (VQ-VAE) and Discrete Variational Autoencoders (DVAE)-not only preserves but enhances explainability by reducing redundancy and focusing on the most informative patterns. We show that applying XAI methods to these compressed representations leads to concise and structured explanations that maintain faithfulness without sacrificing classification performance. Additionally, we propose Similar Subsequence Accuracy (SSA), a novel metric that quantitatively assesses the alignment between XAI-identified salient subsequences and the label distribution in the training data. SSA provides a systematic way to validate whether the features highlighted by XAI methods are truly representative of the learned classification patterns. Our findings demonstrate that discrete latent representations not only retain the essential characteristics needed for classification but also offer a pathway to more compact, interpretable, and computationally efficient explanations in time series analysis.",
      "published": "February 13, 2026",
      "published_raw": "2026-02-13T16:47:45Z",
      "pdf_link": "http://arxiv.org/pdf/2602.13087v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.13087v1"
    }
  ]
}