{
  "last_updated": "2026-02-27T01:04:09.353494",
  "count": 20,
  "papers": [
    {
      "id": "2602.22015v1",
      "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
      "authors": [
        "Pengcheng Hao",
        "Ercan Engin Kuruoglu"
      ],
      "abstract": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T15:29:44Z",
      "pdf_link": "http://arxiv.org/pdf/2602.22015v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.22015v1"
    },
    {
      "id": "2602.21961v1",
      "title": "Robustness in sparse artificial neural networks trained with adaptive topology",
      "authors": [
        "Bendegúz Sulyok",
        "Gergely Palla",
        "Filippo Radicchi",
        "Santo Fortunato"
      ],
      "abstract": "We investigate the robustness of sparse artificial neural networks trained with adaptive topology. We focus on a simple yet effective architecture consisting of three sparse layers with 99% sparsity followed by a dense layer, applied to image classification tasks such as MNIST and Fashion MNIST. By updating the topology of the sparse layers between each epoch, we achieve competitive accuracy despite the significantly reduced number of weights. Our primary contribution is a detailed analysis of the robustness of these networks, exploring their performance under various perturbations including random link removal, adversarial attack, and link weight shuffling. Through extensive experiments, we demonstrate that adaptive topology not only enhances efficiency but also maintains robustness. This work highlights the potential of adaptive sparse networks as a promising direction for developing efficient and reliable deep learning models.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T14:44:15Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21961v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21961v1"
    },
    {
      "id": "2602.21876v1",
      "title": "Comparative Evaluation of Machine Learning Models for Predicting Donor Kidney Discard",
      "authors": [
        "Peer Schliephacke",
        "Hannah Schult",
        "Leon Mizera",
        "Judith Würfel",
        "Gunter Grieser",
        "Axel Rahmel",
        "Carl-Ludwig Fischer-Fröhlich",
        "Antje Jahn-Eimermacher"
      ],
      "abstract": "A kidney transplant can improve the life expectancy and quality of life of patients with end-stage renal failure. Even more patients could be helped with a transplant if the rate of kidneys that are discarded and not transplanted could be reduced. Machine learning (ML) can support decision-making in this context by early identification of donor organs at high risk of discard, for instance to enable timely interventions to improve organ utilization such as rescue allocation. Although various ML models have been applied, their results are difficult to compare due to heterogenous datasets and differences in feature engineering and evaluation strategies. This study aims to provide a systematic and reproducible comparison of ML models for donor kidney discard prediction. We trained five commonly used ML models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Deep Learning along with an ensemble model on data from 4,080 deceased donors (death determined by neurologic criteria) in Germany. A unified benchmarking framework was implemented, including standardized feature engineering and selection, and Bayesian hyperparameter optimization. Model performance was assessed for discrimination (MCC, AUC, F1), calibration (Brier score), and explainability (SHAP). The ensemble achieved the highest discrimination performance (MCC=0.76, AUC=0.87, F1=0.90), while individual models such as Logistic Regression, Random Forest, and Deep Learning performed comparably and better than Decision Trees. Platt scaling improved calibration for tree-and neural network-based models. SHAP consistently identified donor age and renal markers as dominant predictors across models, reflecting clinical plausibility. This study demonstrates that consistent data preprocessing, feature selection, and evaluation can be more decisive for predictive success than the choice of the ML algorithm.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T13:00:05Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21876v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21876v1"
    },
    {
      "id": "2602.21757v1",
      "title": "Learning from Yesterday's Error: An Efficient Online Learning Method for Traffic Demand Prediction",
      "authors": [
        "Xiannan Huang",
        "Quan Yuan",
        "Chao Yang"
      ],
      "abstract": "Accurately predicting short-term traffic demand is critical for intelligent transportation systems. While deep learning models achieve strong performance under stationary conditions, their accuracy often degrades significantly when faced with distribution shifts caused by external events or evolving urban dynamics. Frequent model retraining to adapt to such changes incurs prohibitive computational costs, especially for large-scale or foundation models. To address this challenge, we propose FORESEE (Forecasting Online with Residual Smoothing and Ensemble Experts), a lightweight online adaptation framework that is accurate, robust, and computationally efficient. FORESEE operates without any parameter updates to the base model. Instead, it corrects today's forecast in each region using yesterday's prediction error, stabilized through exponential smoothing guided by a mixture-of-experts mechanism that adapts to recent error dynamics. Moreover, an adaptive spatiotemporal smoothing component propagates error signals across neighboring regions and time slots, capturing coherent shifts in demand patterns. Extensive experiments on seven real-world datasets with three backbone models demonstrate that FORESEE consistently improves prediction accuracy, maintains robustness even when distribution shifts are minimal (avoiding performance degradation), and achieves the lowest computational overhead among existing online methods. By enabling real-time adaptation of traffic forecasting models with negligible computational cost, FORESEE paves the way for deploying reliable, up-to-date prediction systems in dynamic urban environments. Code and data are available at https://github.com/xiannanhuang/FORESEE",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T10:19:39Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21757v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21757v1"
    },
    {
      "id": "2602.21707v1",
      "title": "Learning spatially adaptive sparsity level maps for arbitrary convolutional dictionaries",
      "authors": [
        "Joshua Schulz",
        "David Schote",
        "Christoph Kolbitsch",
        "Kostas Papafitsoros",
        "Andreas Kofler"
      ],
      "abstract": "State-of-the-art learned reconstruction methods often rely on black-box modules that, despite their strong performance, raise questions about their interpretability and robustness. Here, we build on a recently proposed image reconstruction method, which is based on embedding data-driven information into a model-based convolutional dictionary regularization via neural network-inferred spatially adaptive sparsity level maps. By means of improved network design and dedicated training strategies, we extend the method to achieve filter-permutation invariance as well as the possibility to change the convolutional dictionary at inference time. We apply our method to low-field MRI and compare it to several other recent deep learning-based methods, also on in vivo data, in which the benefit for the use of a different dictionary is showcased. We further assess the method's robustness when tested on in- and out-of-distribution data. When tested on the latter, the proposed method suffers less from the data distribution shift compared to the other learned methods, which we attribute to its reduced reliance on training data due to its underlying model-based reconstruction component.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T09:13:24Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21707v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21707v1"
    },
    {
      "id": "2602.21703v1",
      "title": "Brain Tumor Segmentation with Special Emphasis on the Non-Enhancing Brain Tumor Compartment",
      "authors": [
        "T. Schaffer",
        "A. Brawanski",
        "S. Wein",
        "A. M. Tomé",
        "E. W. Lang"
      ],
      "abstract": "A U-Net based deep learning architecture is designed to segment brain tumors as they appear on various MRI modalities. Special emphasis is lent to the non-enhancing tumor compartment. The latter has not been considered anymore in recent brain tumor segmentation challenges like the MICCAI challenges. However, it is considered to be indicative of the survival time of the patient as well as of areas of further tumor growth. Hence it deems essential to have means to automatically delineate its extension within the tumor.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T09:09:12Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21703v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21703v1"
    },
    {
      "id": "2602.21507v1",
      "title": "Conditional Image Diffusion with Interferometric Closure Invariants: Independent EHT Imaging of Centaurus~A and 3C~279",
      "authors": [
        "Samuel Lai",
        "Nithyanandan Thyagarajan",
        "O. Ivy Wong",
        "Foivos Diakogiannis"
      ],
      "abstract": "We present independent imaging analyses of Event Horizon Telescope (EHT) observations of the active galactic nuclei in radio galaxy Centaurus~A and quasar 3C~279 using Generative Deep learning Image Reconstruction with Closure Terms (GenDIReCT), a recently developed machine-learning framework built on conditional diffusion models that uses interferometric closure invariants as primary observables. For Centaurus~A, our reconstruction reveals two prominent emission ridges ($\\simeq 80\\,μ$as each) along the jet sheath with a brightness ratio of $1.4\\pm 0.1$ and an opening angle of $12.3\\pm 0.3$~deg. For 3C~279, we identify three distinct components in the image, with the southern jet ejecta on sub-parsec scale exhibiting a proper motion of $4.6\\pm 1.0\\,μ$as over $\\approx 5.39$ days away from the northern components, corresponding to an apparent superluminal velocity of $\\simeq 10\\pm 2$ times light speed. These measurements are consistent with those reported by the EHT Collaboration. The results are significant because we demonstrate that: (1) imaging from interferometric aperture synthesis data, especially in VLBI and most acutely in extremely sparse arrays like the EHT, remains a severely ill-posed and challenging inverse problem, yet closure invariants preserve robust morphological information that can strongly constrain structural features, and (2) more importantly, closure-invariant imaging largely avoids calibration systematics, thus providing a fundamentally independent view of spatial structure with very high angular resolution. The generative nature of GenDIReCT further allows us to sample and characterise clusters of plausible image solutions for each dataset. As a calibration-independent, generative imaging approach, GenDIReCT offers a robust and truly independent blind-imaging tool for current and future VLBI experiments.",
      "published": "February 25, 2026",
      "published_raw": "2026-02-25T02:37:22Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21507v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21507v1"
    },
    {
      "id": "2602.21415v1",
      "title": "Benchmarking State Space Models, Transformers, and Recurrent Networks for US Grid Forecasting",
      "authors": [
        "Sunki Hong",
        "Jisoo Lee",
        "Yuanyuan Shi"
      ],
      "abstract": "Selecting the right deep learning model for power grid forecasting is challenging, as performance heavily depends on the data available to the operator. This paper presents a comprehensive benchmark of five modern neural architectures: two state space models (PowerMamba, S-Mamba), two Transformers (iTransformer, PatchTST), and a traditional LSTM. We evaluate these models on hourly electricity demand across six diverse US power grids for forecast windows between 24 and 168 hours. To ensure a fair comparison, we adapt each model with specialized temporal processing and a modular layer that cleanly integrates weather covariates. Our results reveal that there is no single best model for all situations. When forecasting using only historical load, PatchTST and the state space models provide the highest accuracy. However, when explicit weather data is added to the inputs, the rankings reverse: iTransformer improves its accuracy three times more efficiently than PatchTST. By controlling for model size, we confirm that this advantage stems from the architecture's inherent ability to mix information across different variables. Extending our evaluation to solar generation, wind power, and wholesale prices further demonstrates that model rankings depend on the forecast task: PatchTST excels on highly rhythmic signals like solar, while state space models are better suited for the chaotic fluctuations of wind and price. Ultimately, this benchmark provides grid operators with actionable guidelines for selecting the optimal forecasting architecture based on their specific data environments.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T22:42:39Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21415v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21415v1"
    },
    {
      "id": "2602.21307v1",
      "title": "SymTorch: A Framework for Symbolic Distillation of Deep Neural Networks",
      "authors": [
        "Elizabeth S. Z. Tan",
        "Adil Soubki",
        "Miles Cranmer"
      ],
      "abstract": "Symbolic distillation replaces neural networks, or components thereof, with interpretable, closed-form mathematical expressions. This approach has shown promise in discovering physical laws and mathematical relationships directly from trained deep learning models, yet adoption remains limited due to the engineering barrier of integrating symbolic regression into deep learning workflows. We introduce SymTorch, a library that automates this distillation by wrapping neural network components, collecting their input-output behavior, and approximating them with human-readable equations via PySR. SymTorch handles the engineering challenges that have hindered adoption: GPU-CPU data transfer, input-output caching, model serialization, and seamless switching between neural and symbolic forward passes. We demonstrate SymTorch across diverse architectures including GNNs, PINNs and transformer models. Finally, we present a proof-of-concept for accelerating LLM inference by replacing MLP layers with symbolic surrogates, achieving an 8.3\\% throughput improvement with moderate performance degradation.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T19:17:56Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21307v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21307v1"
    },
    {
      "id": "2602.21160v1",
      "title": "Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions",
      "authors": [
        "Mame Diarra Toure",
        "David A. Stephens"
      ],
      "abstract": "In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=σ_k^{2}/(2μ_k)$, with $μ_k{=}\\mathbb{E}[p_k]$ and $σ_k^2{=}\\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/μ_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\\sum_k C_k \\approx \\mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\\% over MI and 56.2\\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T18:05:51Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21160v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21160v1"
    },
    {
      "id": "2602.21046v1",
      "title": "PIME: Prototype-based Interpretable MCTS-Enhanced Brain Network Analysis for Disorder Diagnosis",
      "authors": [
        "Kunyu Zhang",
        "Yanwu Yang",
        "Jing Zhang",
        "Xiangjie Shi",
        "Shujian Yu"
      ],
      "abstract": "Recent deep learning methods for fMRI-based diagnosis have achieved promising accuracy by modeling functional connectivity networks. However, standard approaches often struggle with noisy interactions, and conventional post-hoc attribution methods may lack reliability, potentially highlighting dataset-specific artifacts. To address these challenges, we introduce PIME, an interpretable framework that bridges intrinsic interpretability with minimal-sufficient subgraph optimization by integrating prototype-based classification and consistency training with structural perturbations during learning. This encourages a structured latent space and enables Monte Carlo Tree Search (MCTS) under a prototype-consistent objective to extract compact minimal-sufficient explanatory subgraphs post-training. Experiments on three benchmark fMRI datasets demonstrate that PIME achieves state-of-the-art performance. Furthermore, by constraining the search space via learned prototypes, PIME identifies critical brain regions that are consistent with established neuroimaging findings. Stability analysis shows 90% reproducibility and consistent explanations across atlases.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T16:04:52Z",
      "pdf_link": "http://arxiv.org/pdf/2602.21046v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.21046v1"
    },
    {
      "id": "2602.20947v1",
      "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation",
      "authors": [
        "Thorbjørn Mosekjær Iversen",
        "Zebin Duan",
        "Frederik Hagelskjær"
      ],
      "abstract": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T14:31:28Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20947v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20947v1"
    },
    {
      "id": "2602.20744v1",
      "title": "Voices of the Mountains: Deep Learning-Based Vocal Error Detection System for Kurdish Maqams",
      "authors": [
        "Darvan Shvan Khairaldeen",
        "Hossein Hassani"
      ],
      "abstract": "Maqam, a singing type, is a significant component of Kurdish music. A maqam singer receives training in a traditional face-to-face or through self-training. Automatic Singing Assessment (ASA) uses machine learning (ML) to provide the accuracy of singing styles and can help learners to improve their performance through error detection. Currently, the available ASA tools follow Western music rules. The musical composition requires all notes to stay within their expected pitch range from start to finish. The system fails to detect micro-intervals and pitch bends, so it identifies Kurdish maqam singing as incorrect even though the singer performs according to traditional rules. Kurdish maqam requires recognizing performance errors within microtonal spaces, which is beyond Western equal temperament. This research is the first attempt to address the mentioned gap. While many error types happen during singing, our focus is on pitch, rhythm, and modal stability errors in the context of Bayati-Kurd. We collected 50 songs from 13 vocalists ( 2-3 hours) and annotated 221 error spans (150 fine pitch, 46 rhythm, 25 modal drift). The data was segmented into 15,199 overlapping windows and converted to log-mel spectrograms. We developed a two-headed CNN-BiLSTM with attention mode to decide whether a window contains an error and to classify it based on the chosen errors. Trained for 20 epochs with early stopping at epoch 10, the model reached a validation macro-F1 of 0.468. On the full 50-song evaluation at a 0.750 threshold, recall was 39.4% and precision 25.8% . Within detected windows, type macro-F1 was 0.387, with F1 of 0.492 (fine pitch), 0.536 (rhythm), and 0.133 (modal drift); modal drift recall was 8.0%. The better performance on common error types shows that the method works, while the poor modal-drift recall shows that more data and balancing are needed.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T10:17:16Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20744v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20744v1"
    },
    {
      "id": "2602.20652v1",
      "title": "DANCE: Doubly Adaptive Neighborhood Conformal Estimation",
      "authors": [
        "Brandon R. Feng",
        "Brian J. Reich",
        "Daniel Beaglehole",
        "Xihaier Luo",
        "David Keetae Park",
        "Shinjae Yoo",
        "Zhechao Huang",
        "Xueyu Mao",
        "Olcay Boz",
        "Jungeum Kim"
      ],
      "abstract": "The recent developments of complex deep learning models have led to unprecedented ability to accurately predict across multiple data representation types. Conformal prediction for uncertainty quantification of these models has risen in popularity, providing adaptive, statistically-valid prediction sets. For classification tasks, conformal methods have typically focused on utilizing logit scores. For pre-trained models, however, this can result in inefficient, overly conservative set sizes when not calibrated towards the target task. We propose DANCE, a doubly locally adaptive nearest-neighbor based conformal algorithm combining two novel nonconformity scores directly using the data's embedded representation. DANCE first fits a task-adaptive kernel regression model from the embedding layer before using the learned kernel space to produce the final prediction sets for uncertainty quantification. We test against state-of-the-art local, task-adapted and zero-shot conformal baselines, demonstrating DANCE's superior blend of set size efficiency and robustness across various datasets.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T07:54:53Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20652v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20652v1"
    },
    {
      "id": "2602.20651v1",
      "title": "Sparse Bayesian Deep Functional Learning with Structured Region Selection",
      "authors": [
        "Xiaoxian Zhu",
        "Yingmeng Li",
        "Shuangge Ma",
        "Mengyun Wu"
      ],
      "abstract": "In modern applications such as ECG monitoring, neuroimaging, wearable sensing, and industrial equipment diagnostics, complex and continuously structured data are ubiquitous, presenting both challenges and opportunities for functional data analysis. However, existing methods face a critical trade-off: conventional functional models are limited by linearity, whereas deep learning approaches lack interpretable region selection for sparse effects. To bridge these gaps, we propose a sparse Bayesian functional deep neural network (sBayFDNN). It learns adaptive functional embeddings through a deep Bayesian architecture to capture complex nonlinear relationships, while a structured prior enables interpretable, region-wise selection of influential domains with quantified uncertainty. Theoretically, we establish rigorous approximation error bounds, posterior consistency, and region selection consistency. These results provide the first theoretical guarantees for a Bayesian deep functional model, ensuring its reliability and statistical rigor. Empirically, comprehensive simulations and real-world studies confirm the effectiveness and superiority of sBayFDNN. Crucially, sBayFDNN excels in recognizing intricate dependencies for accurate predictions and more precisely identifies functionally meaningful regions, capabilities fundamentally beyond existing approaches.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T07:53:59Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20651v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20651v1"
    },
    {
      "id": "2602.20646v1",
      "title": "On the Convergence of Stochastic Gradient Descent with Perturbed Forward-Backward Passes",
      "authors": [
        "Boao Kong",
        "Hengrui Zhang",
        "Kun Yuan"
      ],
      "abstract": "We study stochastic gradient descent (SGD) for composite optimization problems with $N$ sequential operators subject to perturbations in both the forward and backward passes. Unlike classical analyses that treat gradient noise as additive and localized, perturbations to intermediate outputs and gradients cascade through the computational graph, compounding geometrically with the number of operators. We present the first comprehensive theoretical analysis of this setting. Specifically, we characterize how forward and backward perturbations propagate and amplify within a single gradient step, derive convergence guarantees for both general non-convex objectives and functions satisfying the Polyak--Łojasiewicz condition, and identify conditions under which perturbations do not deteriorate the asymptotic convergence order. As a byproduct, our analysis furnishes a theoretical explanation for the gradient spiking phenomenon widely observed in deep learning, precisely characterizing the conditions under which training recovers from spikes or diverges. Experiments on logistic regression with convex and non-convex regularization validate our theories, illustrating the predicted spike behavior and the asymmetric sensitivity to forward versus backward perturbations.",
      "published": "February 24, 2026",
      "published_raw": "2026-02-24T07:47:15Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20646v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20646v1"
    },
    {
      "id": "2602.20324v1",
      "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models",
      "authors": [
        "Cathy Shyr",
        "Yan Hu",
        "Rory J. Tinker",
        "Thomas A. Cassini",
        "Kevin W. Byram",
        "Rizwan Hamid",
        "Daniel V. Fabbri",
        "Adam Wright",
        "Josh F. Peterson",
        "Lisa Bastarache",
        "Hua Xu"
      ],
      "abstract": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T20:20:23Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20324v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20324v1"
    },
    {
      "id": "2602.20303v1",
      "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health",
      "authors": [
        "Joyanta Jyoti Mondal"
      ],
      "abstract": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T19:31:44Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20303v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20303v1"
    },
    {
      "id": "2602.20289v1",
      "title": "The Sim-to-Real Gap in MRS Quantification: A Systematic Deep Learning Validation for GABA",
      "authors": [
        "Zien Ma",
        "S. M. Shermer",
        "Oktay Karakuş",
        "Frank C. Langbein"
      ],
      "abstract": "Magnetic resonance spectroscopy (MRS) is used to quantify metabolites in vivo and estimate biomarkers for conditions ranging from neurological disorders to cancers. Quantifying low-concentration metabolites such as GABA ($γ$-aminobutyric acid) is challenging due to low signal-to-noise ratio (SNR) and spectral overlap. We investigate and validate deep learning for quantifying complex, low-SNR, overlapping signals from MEGA-PRESS spectra, devise a convolutional neural network (CNN) and a Y-shaped autoencoder (YAE), and select the best models via Bayesian optimisation on 10,000 simulated spectra from slice-profile-aware MEGA-PRESS simulations. The selected models are trained on 100,000 simulated spectra. We validate their performance on 144 spectra from 112 experimental phantoms containing five metabolites of interest (GABA, Glu, Gln, NAA, Cr) with known ground truth concentrations across solution and gel series acquired at 3 T under varied bandwidths and implementations. These models are further assessed against the widely used LCModel quantification tool. On simulations, both models achieve near-perfect agreement (small MAEs; regression slopes $\\approx 1.00$, $R^2 \\approx 1.00$). On experimental phantom data, errors initially increased substantially. However, modelling variable linewidths in the training data significantly reduced this gap. The best augmented deep learning models achieved a mean MAE for GABA over all phantom spectra of 0.151 (YAE) and 0.160 (FCNN) in max-normalised relative concentrations, outperforming the conventional baseline LCModel (0.220). A sim-to-real gap remains, but physics-informed data augmentation substantially reduced it. Phantom ground truth is needed to judge whether a method will perform reliably on real data.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T19:16:03Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20289v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20289v1"
    },
    {
      "id": "2602.20271v1",
      "title": "Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning",
      "authors": [
        "Stefan Faulkner",
        "Reza Zandehshahvar",
        "Vahid Eghbal Akhlaghi",
        "Sebastien Ouellet",
        "Carsten Jordan",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.",
      "published": "February 23, 2026",
      "published_raw": "2026-02-23T19:01:03Z",
      "pdf_link": "http://arxiv.org/pdf/2602.20271v1.pdf",
      "arxiv_link": "http://arxiv.org/abs/2602.20271v1"
    }
  ]
}