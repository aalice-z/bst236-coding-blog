<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Latest arXiv Papers - My Coding Blog</title>
    <link rel="stylesheet" href="style.css">
    <style>
        .papers-container {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            min-height: 100vh;
            padding: 2rem 0;
        }

        .papers-header {
            text-align: center;
            color: white;
            margin-bottom: 2rem;
        }

        .papers-header h1 {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: 0.5rem;
        }

        .papers-header p {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        .papers-info {
            background: white;
            padding: 1rem 2rem;
            border-radius: 1rem;
            box-shadow: var(--shadow-lg);
            margin-bottom: 2rem;
            text-align: center;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        .papers-info p {
            margin: 0.5rem 0;
            color: var(--text-light);
        }

        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 2rem;
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        .paper-card {
            background: white;
            border-radius: 1rem;
            padding: 1.5rem;
            box-shadow: var(--shadow);
            transition: transform 0.3s, box-shadow 0.3s;
            display: flex;
            flex-direction: column;
            border: 2px solid transparent;
        }

        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: var(--shadow-lg);
            border-color: var(--primary-color);
        }

        .paper-header {
            margin-bottom: 1rem;
        }

        .paper-title {
            font-size: 1.2rem;
            margin-bottom: 0.5rem;
            line-height: 1.4;
        }

        .paper-title a {
            color: var(--text-color);
            text-decoration: none;
            transition: color 0.3s;
        }

        .paper-title a:hover {
            color: var(--primary-color);
        }

        .paper-meta {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .paper-date {
            color: var(--accent-color);
            font-weight: 600;
        }

        .paper-id a {
            color: var(--text-light);
            text-decoration: none;
            font-family: monospace;
        }

        .paper-id a:hover {
            color: var(--primary-color);
        }

        .paper-authors {
            margin-bottom: 1rem;
            padding: 0.75rem;
            background: var(--bg-secondary);
            border-radius: 0.5rem;
            font-size: 0.9rem;
            color: var(--text-light);
        }

        .authors-label {
            font-weight: 600;
            color: var(--text-color);
        }

        .paper-abstract {
            flex-grow: 1;
            margin-bottom: 1rem;
            line-height: 1.6;
            color: var(--text-light);
        }

        .paper-abstract p {
            margin: 0;
        }

        .paper-links {
            display: flex;
            gap: 1rem;
            margin-top: auto;
        }

        .paper-link {
            flex: 1;
            text-align: center;
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            text-decoration: none;
            font-weight: 600;
            font-size: 0.9rem;
            transition: transform 0.2s, opacity 0.2s;
        }

        .paper-link:hover {
            transform: scale(1.05);
            opacity: 0.9;
        }

        .pdf-link {
            background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));
            color: white;
        }

        .arxiv-link {
            background: var(--bg-secondary);
            color: var(--text-color);
            border: 2px solid var(--border-color);
        }

        .back-link {
            display: inline-block;
            margin: 2rem auto;
            padding: 0.75rem 1.5rem;
            background: rgba(255, 255, 255, 0.2);
            color: white;
            text-decoration: none;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: background 0.3s;
            text-align: center;
        }

        .back-link:hover {
            background: rgba(255, 255, 255, 0.3);
        }

        .back-link-container {
            text-align: center;
            margin-top: 3rem;
        }

        @media (max-width: 768px) {
            .papers-grid {
                grid-template-columns: 1fr;
                padding: 0 1rem;
            }

            .papers-header h1 {
                font-size: 2rem;
            }

            .paper-links {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="papers-container">
        <div class="container">
            <div class="papers-header">
                <h1>ğŸ“š Latest arXiv Papers</h1>
                <p>Curated research papers on machine learning and deep learning</p>
            </div>

            <div class="papers-info">
                <p><strong>20 papers</strong> â€¢ Last updated: February 19, 2026 at 01:04 AM</p>
                <p>ğŸ”„ Automatically updated daily at midnight</p>
            </div>

            <div class="papers-grid">

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15830v1.pdf" target="_blank" rel="noopener noreferrer">
                        Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 17, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15830v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15830v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Christopher David Roberts
            </div>
            <div class="paper-abstract">
                <p>Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either una...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15830v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15830v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15637v1.pdf" target="_blank" rel="noopener noreferrer">
                        The Stationarity Bias: Stratified Stress-Testing for Time-Series Imputation in Regulated Dynamical Systems
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 17, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15637v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15637v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Amirreza Dolatpour Fathkouhi, Alireza Namazi, Heman Shakeri
            </div>
            <div class="paper-abstract">
                <p>Time-series imputation benchmarks employ uniform random masking and shape-agnostic metrics (MSE, RMSE), implicitly weighting evaluation by regime prevalence. In systems with a dominant attractor -- homeostatic physiology, nominal industrial operation, stable network traffic -- this creates a syst...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15637v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15637v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15602v1.pdf" target="_blank" rel="noopener noreferrer">
                        Certified Per-Instance Unlearning Using Individual Sensitivity Bounds
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 17, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15602v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15602v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Hanna Benarroch, Jamal Atif, Olivier CappÃ©
            </div>
            <div class="paper-abstract">
                <p>Certified machine unlearning can be achieved via noise injection leading to differential privacy guarantees, where noise is calibrated to worst-case sensitivity. Such conservative calibration often results in performance degradation, limiting practical applicability. In this work, we investigate ...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15602v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15602v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15552v1.pdf" target="_blank" rel="noopener noreferrer">
                        Latent Regularization in Generative Test Input Generation
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 17, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15552v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15552v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Giorgi Merabishvili, Oliver WeiÃŸl, Andrea Stocco
            </div>
            <div class="paper-abstract">
                <p>This study investigates the impact of regularization of latent spaces through truncation on the quality of generated test inputs for deep learning classifiers. We evaluate this effect using style-based GANs, a state-of-the-art generative approach, and assess quality along three dimensions: validi...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15552v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15552v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15484v1.pdf" target="_blank" rel="noopener noreferrer">
                        Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 17, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15484v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15484v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span>  Amartyaveer, Murali Kadambi, Chandra Mohan Sharma, et al. (5 authors)
            </div>
            <div class="paper-abstract">
                <p>In this study, we have presented a novel approach to predict the Short-Time Objective Intelligibility (STOI) metric using a bottleneck transformer architecture. Traditional methods for calculating STOI typically requires clean reference speech, which limits their applicability in the real world. ...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15484v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15484v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15167v1.pdf" target="_blank" rel="noopener noreferrer">
                        Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15167v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15167v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Xiaoyi Wen, Fei Jiang
            </div>
            <div class="paper-abstract">
                <p>Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct hig...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15167v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15167v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15141v1.pdf" target="_blank" rel="noopener noreferrer">
                        Mimicking the large-scale structure of the Local Universe. Synthetic pre-labelled galaxies in large-scale structures
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15141v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15141v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> M. AlcÃ¡zar-Laynez, S. Duarte Puertas, S. Verley, et al. (9 authors)
            </div>
            <div class="paper-abstract">
                <p>Current observational and simulated large-scale structure (LSS) catalogues often lack consistency in assigning galaxies to specific structures, due to the absence of a universally accepted classification criterion. With the aim to generate synthetic empirical data for fine-tuning LSS classificati...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15141v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15141v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15128v1.pdf" target="_blank" rel="noopener noreferrer">
                        PolyNODE: Variable-dimension Neural ODEs on M-polyfolds
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15128v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15128v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Per Ã…hag, Alexander Friedrich, Fredrik Ohlsson, et al. (4 authors)
            </div>
            <div class="paper-abstract">
                <p>Neural ordinary differential equations (NODEs) are geometric deep learning models based on dynamical systems and flows generated by vector fields on manifolds. Despite numerous successful applications, particularly within the flow matching paradigm, all existing NODE models are fundamentally cons...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15128v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15128v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14997v1.pdf" target="_blank" rel="noopener noreferrer">
                        Spectral Convolution on Orbifolds for Geometric Deep Learning
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14997v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14997v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Tim Mangliers, Bernhard MÃ¶ssner, Benjamin Himpel
            </div>
            <div class="paper-abstract">
                <p>Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures wit...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14997v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14997v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14939v1.pdf" target="_blank" rel="noopener noreferrer">
                        Fault Detection in Electrical Distribution System using Autoencoders
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14939v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14939v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Sidharthenee Nayak, Victor Sam Moses Babu, Chandrashekhar Narayan Bhende, et al. (5 authors)
            </div>
            <div class="paper-abstract">
                <p>In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their pr...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14939v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14939v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14896v1.pdf" target="_blank" rel="noopener noreferrer">
                        Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14896v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14896v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Pedram Bakhtiarifard, Tong Chen, Jonathan WenshÃ¸j, et al. (5 authors)
            </div>
            <div class="paper-abstract">
                <p>Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14896v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14896v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14853v1.pdf" target="_blank" rel="noopener noreferrer">
                        BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14853v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14853v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Jonathan Gorard, Ammar Hakim, James Juno
            </div>
            <div class="paper-abstract">
                <p>The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically valida...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14853v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14853v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14833v1.pdf" target="_blank" rel="noopener noreferrer">
                        RF-GPT: Teaching AI to See the Wireless World
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14833v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14833v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Hang Zou, Yu Tian, Bohao Wang, et al. (7 authors)
            </div>
            <div class="paper-abstract">
                <p>Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and s...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14833v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14833v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.15089v1.pdf" target="_blank" rel="noopener noreferrer">
                        Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.15089v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.15089v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Takato Yasuno
            </div>
            <div class="paper-abstract">
                <p>In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional t...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.15089v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.15089v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14759v1.pdf" target="_blank" rel="noopener noreferrer">
                        Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14759v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14759v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Jonathan Lys, Vincent Gripon, Bastien Pasdeloup, et al. (6 authors)
            </div>
            <div class="paper-abstract">
                <p>Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner rep...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14759v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14759v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14701v1.pdf" target="_blank" rel="noopener noreferrer">
                        Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14701v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14701v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Killian Bakong, Laurent MassouliÃ©, Edouard Oyallon, et al. (4 authors)
            </div>
            <div class="paper-abstract">
                <p>In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14701v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14701v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14635v1.pdf" target="_blank" rel="noopener noreferrer">
                        Alignment Adapter to Improve the Performance of Compressed Deep Learning Models
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14635v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14635v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Rohit Raj Rai, Abhishek Dhaka, Amit Awekar
            </div>
            <div class="paper-abstract">
                <p>Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14635v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14635v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14519v1.pdf" target="_blank" rel="noopener noreferrer">
                        DeepMTL2R: A Library for Deep Multi-task Learning to Rank
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14519v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14519v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Chaosheng Dong, Peiyao Xiao, Yijia Wang, et al. (4 authors)
            </div>
            <div class="paper-abstract">
                <p>This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attent...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14519v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14519v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14445v1.pdf" target="_blank" rel="noopener noreferrer">
                        Selective Synchronization Attention
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 16, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14445v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14445v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Hasi Hays
            </div>
            <div class="paper-abstract">
                <p>The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mecha...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14445v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14445v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    

        <div class="paper-card">
            <div class="paper-header">
                <h3 class="paper-title">
                    <a href="http://arxiv.org/pdf/2602.14239v1.pdf" target="_blank" rel="noopener noreferrer">
                        A Hybrid TGN-SEAL Model for Dynamic Graph Link Prediction
                    </a>
                </h3>
                <div class="paper-meta">
                    <span class="paper-date">ğŸ“… February 15, 2026</span>
                    <span class="paper-id">
                        <a href="http://arxiv.org/abs/2602.14239v1" target="_blank" rel="noopener noreferrer">
                            arXiv:2602.14239v1
                        </a>
                    </span>
                </div>
            </div>
            <div class="paper-authors">
                <span class="authors-label">ğŸ‘¥ Authors:</span> Nafiseh Sadat Sajadi, Behnam Bahrak, Mahdi Jafari Siavoshani
            </div>
            <div class="paper-abstract">
                <p>Predicting links in sparse, continuously evolving networks is a central challenge in network science. Conventional heuristic methods and deep learning models, including Graph Neural Networks (GNNs), are typically designed for static graphs and thus struggle to capture temporal dependencies. Snaps...</p>
            </div>
            <div class="paper-links">
                <a href="http://arxiv.org/pdf/2602.14239v1.pdf" class="paper-link pdf-link" target="_blank" rel="noopener noreferrer">
                    ğŸ“„ PDF
                </a>
                <a href="http://arxiv.org/abs/2602.14239v1" class="paper-link arxiv-link" target="_blank" rel="noopener noreferrer">
                    ğŸ”— arXiv Page
                </a>
            </div>
        </div>
    
            </div>

            <div class="back-link-container">
                <a href="index.html" class="back-link">â† Back to Home</a>
            </div>
        </div>
    </div>
</body>
</html>